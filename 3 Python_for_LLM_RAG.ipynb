{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33bd405a",
   "metadata": {},
   "source": [
    "# Basic LLM Calls and RAG Workflow\n",
    "## Quick Start Guide for Large Language Models (LLMs)\n",
    "\n",
    "For practice purposes, I use OpenAI's GPT-4o-mini models for text based Q&A. \n",
    "\n",
    "All environment variables are set in the `.env` file, which is not included in this repository for security reasons.\n",
    "\n",
    "The basic LLM call is made using the `openai` Python package, which is installed via pip. \n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "client = openai.Client(api_key=\"your-api-key\",base_url= 'your-base-url')  # Set your OpenAI API key and base URL\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o-mini\", # or \"gpt-4o-mini-preview\" for the preview version\n",
    "    temperature=0.7, # controls randomness in the response\n",
    "    max_tokens=100, # maximum number of tokens in the response\n",
    "    top_p=1.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    ")\n",
    "message = \"What is the capital of France?\"\n",
    "messages = \"You are a helpful assistant. Please answer the question: \" + message\n",
    "print(response.choices[0].message.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional\n",
    "!pip install openai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027df6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Set API key and base URL globally\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") # Use correct env var name from your .env or manually set it\n",
    "print(\"OpenAI API Key:\", api_key[:10] + \"...\" if api_key else \"Not found\")  # Only show first 10 chars for security\n",
    "base_url = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")  # Use correct env var name from your .env\n",
    "print(\"OpenAI Base URL:\", base_url)\n",
    "\n",
    "# Initialize OpenAI client with API key and base URL\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7bd0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the OpenAI client\n",
    "message = \"What is the capital of France?\"\n",
    "\n",
    "messages = \"You are a helpful assistant. Please answer the question: \" + message \n",
    "#this is the final prompt sent\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", # or \"gpt-4o-mini-preview\" for the preview version\n",
    "    temperature=0.7, # controls randomness in the response\n",
    "    max_tokens=100, # maximum number of tokens in the response\n",
    "    top_p=1.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "# this will return the response from the model, which should be \"Paris\" for this question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcae0b92",
   "metadata": {},
   "source": [
    "LLMs can answer quetions about non-text data, such as images, audio, and video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question from picture with 4o and image\n",
    "image_url ='https://static.independent.co.uk/s3fs-public/thumbnails/image/2017/03/28/13/kitten.jpg?quality=75&width=1250&crop=3%3A2%2Csmart&auto=webp'\n",
    "\n",
    "response_pic = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    "    top_p=1.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": [{\"type\": \"text\", \"text\": \"What is this a picture of?\"}, \n",
    "                     {\"type\": \"image_url\", \"image_url\": image_url}]\n",
    "            }\n",
    "        ]\n",
    ")\n",
    "print(response_pic.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648eab63",
   "metadata": {},
   "source": [
    "## LLM Calls with Multiple Rounds\n",
    "This is useful for more complex interactions where the model needs to maintain context over multiple exchanges.\n",
    "\n",
    "Different from the single round call, we need to maintain a list of messages that represent the conversation history. The messages list is constructed with alternating user and assistant roles.\n",
    "\n",
    "```python\n",
    "import openai\n",
    "client = openai.Client(api_key=\"your-api-key\",base_url= 'your-base-url')  # Set your OpenAI API key and base URL\n",
    "\n",
    "def multi_round_chat(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", # or \"gpt-4o-mini-preview\" for the preview version\n",
    "        temperature=0.7, # controls randomness in the response\n",
    "        max_tokens=100, # maximum number of tokens in the response\n",
    "        top_p=1.0,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content  \n",
    "\n",
    "user_input = input(\"You: \")\n",
    "messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "while True:\n",
    "    response = multi_round_chat(messages)\n",
    "    print(\"Assistant:\", response)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    user_input = input(\"You: \")\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4bcafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") # Use correct env var name from your .env or manually set it\n",
    "print(\"OpenAI API Key:\", api_key[:10] + \"...\" if api_key else \"Not found\")  # Only show first 10 chars for security\n",
    "base_url = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")  # Use correct env var name from your .env\n",
    "print(\"OpenAI Base URL:\", base_url)\n",
    "\n",
    "# Initialize OpenAI client with API key and base URL\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_round_chat(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", # or \"gpt-4o-mini-preview\" for the preview version\n",
    "        temperature=0.7, # controls randomness in the response\n",
    "        max_tokens=400, # maximum number of tokens in the response\n",
    "        top_p=1.0,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content  \n",
    "\n",
    "user_input = input(\"You: \")\n",
    "print(\"You:\", user_input)\n",
    "# Initialize messages with the user's input\n",
    "messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "for _ in range(3):\n",
    "    response = multi_round_chat(messages)\n",
    "    print(\"Assistant:\")\n",
    "    display(Markdown(response)) \n",
    "    print(\"-\" * 50) \n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    user_input = input(\"You: \")\n",
    "    print(\"You:\", user_input)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9997bc",
   "metadata": {},
   "source": [
    "The code above allows for a multi-round conversation with the LLM. The user can input a message, and the assistant will respond based on the conversation history. The loop continues until the user decides to stop. It can be viewed as a simple chat interface with the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8cbad9",
   "metadata": {},
   "source": [
    "## Structure the Response\n",
    "A large quantity of data are stored in a structured format, such as JSON or XML. LLMs can also return structured data in the response.\n",
    "```python\n",
    "house_data = [\n",
    "    {\n",
    "        \"address\": \"123 Main St\",\n",
    "        \"price\": 500000,\n",
    "        \"bedrooms\": 3,\n",
    "        \"bathrooms\": 2,\n",
    "        \"features\": [\"garage\", \"garden\"]\n",
    "    },\n",
    "    {\n",
    "        \"address\": \"456 Elm St\",\n",
    "        \"price\": 600000,\n",
    "        \"bedrooms\": 4,\n",
    "        \"bathrooms\": 3,\n",
    "        \"features\": [\"pool\", \"fireplace\"]\n",
    "    }\n",
    "]\n",
    "```\n",
    "This is a simple example of how to structure the response from an LLM. With f-strings, you can easily format the output to include variable data in a readable way. For example:\n",
    "```python\n",
    "description = f\"The house at {house_data[0]['address']} has {house_data[0]['bedrooms']} bedrooms and is priced at ${house_data[0]['price']}. It features a {', '.join(house_data[0]['features'])}.\"\n",
    "\n",
    "print(description)\n",
    "```\n",
    "This will output:\n",
    "```\n",
    "The house at 123 Main St has 3 bedrooms and is priced at $500000. It features a garage, garden.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc438176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data for a real estate listing\n",
    "house_data = [\n",
    "    {\"address\": \"123 Main St, Springfield\", \"price\": 500000, \"bedrooms\": 3, \"bathrooms\": 2, \"features\": [\"garage\", \"garden\"]},\n",
    "    {\"address\": \"456 Elm St, Springfield\", \"price\": 600000, \"bedrooms\": 4, \"bathrooms\": 3, \"features\": [\"pool\", \"fireplace\"]},\n",
    "    {\"address\": \"789 Oak St, Springfield\", \"price\": 450000, \"bedrooms\": 2, \"bathrooms\": 1, \"features\": [\"fenced yard\", \"new roof\"]},\n",
    "    {\"address\": \"101 Pine St, Springfield\", \"price\": 700000, \"bedrooms\": 5, \"bathrooms\": 4, \"features\": [\"home office\", \"basement\"]},\n",
    "    {\"address\": \"202 Maple St, Springfield\", \"price\": 550000, \"bedrooms\": 3, \"bathrooms\": 2, \"features\": [\"deck\", \"modern kitchen\"]},\n",
    "]\n",
    "# Accessing the data with simple print and formatting\n",
    "print(\"Real Estate Listings:\")\n",
    "for house in house_data:\n",
    "    print(f\"Address: {house['address']}, Price: ${house['price']}, Bedrooms: {house['bedrooms']}, Bathrooms: {house['bathrooms']}, Features: {', '.join(house['features'])}\")\n",
    "    print(\"-\" * 50)  # Separator for readability\n",
    "\n",
    "# Improved formatting with descriptive text\n",
    "def house_info(houses):\n",
    "    layout = ''\n",
    "    for house in houses:\n",
    "        layout += f\"House located at {house['address']} is priced at ${house['price']}. It has {house['bedrooms']} bedrooms and {house['bathrooms']} bathrooms. Notable features include: {', '.join(house['features'])}.\\n\"\n",
    "        layout += \"=*=\" * 20 + \"\\n\"  # Separator for readability\n",
    "    return layout\n",
    "print(\"Formatted Real Estate Listings:\")\n",
    "formatted_info = house_info(house_data)\n",
    "print(formatted_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f0475",
   "metadata": {},
   "source": [
    "## Putting It All Together (LLM + Structured Data)\n",
    "You can combine the LLM calls with structured data to create a more complex application. For example, you can use the LLM to generate a summary of a dataset or to answer questions about the data.\n",
    "\n",
    "The workflow can be summarized as follows:\n",
    "1. **Load the data**: Load the structured data from a file or database.\n",
    "2. **Process the data**: Use Python to process the data and prepare it for the\n",
    "3. **Call the LLM**: Use the processed data as input to the LLM, either as part of the prompt or as a separate message in a multi-round conversation.   \n",
    "\n",
    "### Diagram\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Load Data] --> B[Process Data]        \n",
    "    B --> C[Call LLM]\n",
    "    C --> D[Receive Response]\n",
    "    D --> E[Display Result]\n",
    "    E --> F[User Input]\n",
    "    F --> C\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2709cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: sk-nzH4YCa...\n",
      "OpenAI Base URL: https://xiaoai.plus/v1\n"
     ]
    }
   ],
   "source": [
    "# Initializing\n",
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") # Use correct env var name from your .env or manually set it\n",
    "print(\"OpenAI API Key:\", api_key[:10] + \"...\" if api_key else \"Not found\")  # Only show first 10 chars for security\n",
    "base_url = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")  # Use correct env var name from your .env\n",
    "print(\"OpenAI Base URL:\", base_url)\n",
    "\n",
    "# Initialize OpenAI client with API key and base URL\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585641c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: any house in china\n",
      "Assistant:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "house_data = [\n",
    "    {\"address\": \"123 Main St, Springfield\", \"price\": 500000, \"bedrooms\": 3, \"bathrooms\": 2, \"features\": [\"garage\", \"garden\"]},\n",
    "    {\"address\": \"456 Elm St, Springfield\", \"price\": 600000, \"bedrooms\": 4, \"bathrooms\": 3, \"features\": [\"pool\", \"fireplace\"]},\n",
    "    {\"address\": \"789 Oak St, Springfield\", \"price\": 450000, \"bedrooms\": 2, \"bathrooms\": 1, \"features\": [\"fenced yard\", \"new roof\"]},\n",
    "    {\"address\": \"101 Pine St, Springfield\", \"price\": 700000, \"bedrooms\": 5, \"bathrooms\": 4, \"features\": [\"home office\", \"basement\"]},\n",
    "    {\"address\": \"202 Maple St, Springfield\", \"price\": 550000, \"bedrooms\": 3, \"bathrooms\": 2, \"features\": [\"deck\", \"modern kitchen\"]},\n",
    "]\n",
    "def house_info(houses):\n",
    "    layout = ''\n",
    "    for house in houses:\n",
    "        layout += f\"House located at {house['address']} is priced at ${house['price']}. It has {house['bedrooms']} bedrooms and {house['bathrooms']} bathrooms. Notable features include: {', '.join(house['features'])}.\\n\"\n",
    "        layout += \"=*=\" * 20 + \"\\n\"  # Separator for readability\n",
    "    return layout\n",
    "\n",
    "# Define a function to call LLM\n",
    "def sys_prompt(house_data, user_query):\n",
    "    prompt = \"You are a real estate assistant. Use the following houses information to answer users queries:\\n\"\n",
    "    prompt += house_info(house_data)\n",
    "    prompt += \"\\nNow, answer the user's query based on the provided house information. If you don't know the answer, say 'I don't know'. \\n\"\n",
    "    prompt += f\"User Query: {user_query}\\n\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_llm_response(prompt, api_key=api_key, base_url=base_url):\n",
    "        client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4o-mini-preview\" for the preview version\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "        top_p=1.0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "user_input = input(\"You: \")\n",
    "print(\"You:\", user_input)\n",
    "# Initialize messages with the user's input\n",
    "messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "# Generate response using the LLM with data\n",
    "response = generate_llm_response (sys_prompt(house_data, user_input), api_key=api_key, base_url=base_url)\n",
    "print(\"Assistant:\")\n",
    "display(Markdown(response))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
