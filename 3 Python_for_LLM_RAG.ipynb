{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33bd405a",
   "metadata": {},
   "source": [
    "# Basic LLM Calls and RAG Workflow\n",
    "## Quick Start Guide for Large Language Models (LLMs)\n",
    "\n",
    "For practice purposes, I use OpenAI's GPT-4o-mini models for text based Q&A.  In this exercise, I will show how to make basic LLM calls and implement a Retrieval-Augmented Generation (RAG) workflow using Python. The \"database\" used in this example is a simple structured dummy dataset, which can be easily replaced with any other data source (Notebook 5).\n",
    "\n",
    "All environment variables are set in the `.env` file, which is not included in this repository for security reasons. A brief description of the `.env` file is provided below.\n",
    "\n",
    "The basic LLM call is made using the `openai` Python package, which is installed via pip. \n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "client = openai.Client(api_key=\"your-api-key\",base_url= 'your-base-url')  # Set your OpenAI API key and base URL\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o-mini\", # or \"gpt-5-nano\"\n",
    "    temperature=0.7, # controls randomness in the response\n",
    "    max_tokens=100, # maximum number of tokens in the response\n",
    "    top_p=1.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    ")\n",
    "message = \"What is the capital of France?\"\n",
    "messages = \"You are a helpful assistant. Please answer the question: \" + message\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "### Getting started with OpenAI Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional\n",
    "!pip install openai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ff4f3",
   "metadata": {},
   "source": [
    "### Set up the OpenAI client\n",
    "\n",
    "With the latest OpenAI Python SDK, the way to call API has changed compared to older versions. The key difference is:\n",
    "\n",
    "- You must create a client instance (OpenAI()) to call the API.\n",
    "\n",
    "- The client instance will automatically read API key and base URL from environment variables if you donâ€™t pass them explicitly.\n",
    "\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"your-api-key\", api_base=\"your-base-url\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd6279d",
   "metadata": {},
   "source": [
    "### Getting Environment Variables from the .env File\n",
    "It is a good practice to store sensitive information like API keys in environment variables. You can use the `dotenv` or os package to load these variables from a `.env` file.\n",
    "\n",
    "You should avoid hardcoding sensitive information in your code. Instead, use environment variables to store them securely.\n",
    "\n",
    "A typical `.env` file might look like this:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your-api-key\n",
    "OPENAI_API_BASE_URL=your-base-url\n",
    "other_variable=value\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027df6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: sk-nzH4YCa...\n",
      "OpenAI Base URL: https://xiaoai.plus/v1\n",
      "OpenAI Model: gpt-5-nano\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Set API key and base URL globally\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") # Use correct env var name from your .env or manually set it\n",
    "print(\"OpenAI API Key:\", api_key[:10] + \"...\" if api_key else \"Not found\")  # Only show first 10 chars for security\n",
    "\n",
    "base_url = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")  # Use correct env var name from your .env\n",
    "print(\"OpenAI Base URL:\", base_url)\n",
    "\n",
    "model = os.getenv(\"OPENAI_MODEL\", \"gpt-5-nano\")  # Use correct env var name from your .env\n",
    "print(\"OpenAI Model:\", model)  # Print the model being used\n",
    "\n",
    "# Initialize OpenAI client with API key and base URL\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3608dc6",
   "metadata": {},
   "source": [
    "### Calling the OpenAI API\n",
    "To get a response from the OpenAI API, you can use the `client.chat.completions.create` method. This method takes several parameters, including the model to use, the temperature, and the messages to send to the model.\n",
    "\n",
    "The basic syntax for calling the OpenAI API is as follows:\n",
    "\n",
    "Step 1: Import the necessary libraries and set up the OpenAI client.\n",
    "\n",
    "Step 2: Create a client instance with your API key and base URL.\n",
    "\n",
    "`Step 3: Define a function to generate a response from the LLM, optionally using RAG data.`\n",
    "\n",
    "There are several critical parameters to consider when making an LLM call:\n",
    "- `model`: The model to use for the LLM call, such as \"gpt-4o-mini\" or \"gpt-4o-mini-preview\". This determines the capabilities, performance and cost of the LLM.\n",
    "\n",
    "- `max_tokens`: The maximum number of tokens to generate in the response. This limits the length of the response and helps control costs. Not available in the gpt-5-nano model.\n",
    "\n",
    "- `top_p`: Controls the diversity of the response. A value of 1.0 means no filtering, while lower values (e.g., 0.9) restrict the response to more probable tokens. For example, setting `top_p=0.9` will only consider the top k most likely tokens for the response, which can help reduce randomness and improve coherence. If you set `top_p` to 0, the model will always choose the most likely next token, resulting in very deterministic responses.\n",
    "\n",
    "- `messages`: A list of messages to send to the model. Each message is a dictionary with a `role` (either \"user\" or \"assistant\") and `content` (the text of the message). This allows for multi-turn conversations and context management.\n",
    "\n",
    "- `temperature`: The temperature parameter in a language model (LLM) is a **scalar** value that controls the randomness of the model's predictions. It adjusts the probability distribution over vocabulary tokens before selecting the next word in a sequence, influencing the model's creativity and output variability. Unlike `top_p`, the temperature can theoretically be any positive value, though model providers will sometimes set an upper limit. A higher temperature (e.g., 0.7) results in more creative responses, while a lower temperature (e.g., 0.2) results in more focused and deterministic responses. \n",
    "\n",
    "- `repetition_penalty`: A parameter that discourages the model from repeating the same phrases or words in its response. A higher value (e.g., 1.2) increases the penalty for repetition, which can help produce more varied and interesting responses. **Not available in the gpt models though.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d427f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from GPT-4o-mini: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Example of using GPT-4o-mini\n",
    "message = \"What is the capital of France?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": message}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    max_tokens=500,  # Limit response length\n",
    "    temperature=0.7,  # Control randomness\n",
    "    top_p=0.5,  # Use all tokens\n",
    "    \n",
    ")\n",
    "print(\"Response from GPT-4o-mini:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7bd0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris.\n",
      "Model used is: gpt-5-nano\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the OpenAI client with gpt-5-nano model\n",
    "# This example assumes you have set the OPENAI_API_KEY and OPENAI_API_BASE environment variables correctly.\n",
    "# If you haven't set them, you can replace them with your actual API key and base URL.\n",
    "message = \"What is the capital of France?\"\n",
    "\n",
    "messages = \"You are a helpful assistant. Please answer the question: \" + message \n",
    "#this is the final prompt sent\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model= model, # or \"gpt-5-nano\" for the preview version\n",
    "    # temperature and max_tokens are not supported for gpt-5-nano\n",
    "    # temperature=0.7, # controls randomness in the response (not supported for gpt-5-nano)\n",
    "    # max_tokens=100, # maximum number of tokens in the response (not supported for gpt-5-nano)\n",
    "    top_p=1.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(\"Model used is:\", model)\n",
    "# this will return the response from the model, which should be \"Paris\" for this question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcae0b92",
   "metadata": {},
   "source": [
    "LLMs can answer quetions about non-text data, such as images, audio, and video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d839b0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A kitten (cat) inside a cardboard box with its mouth open.\n"
     ]
    }
   ],
   "source": [
    "# Answer question from picture with 5-nano and image\n",
    "image_url ='https://static.independent.co.uk/s3fs-public/thumbnails/image/2017/03/28/13/kitten.jpg?quality=75&width=1250&crop=3%3A2%2Csmart&auto=webp'\n",
    "\n",
    "response_pic = client.chat.completions.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    # temperature and max_tokens are not supported for gpt-5-nano\n",
    "    # temperature=0.7, # not supported for gpt-5-nano\n",
    "    # max_tokens=300, # not supported for gpt-5-nano\n",
    "    top_p=1.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": [{\"type\": \"text\", \"text\": \"What is this a picture of?\"}, \n",
    "                     {\"type\": \"image_url\", \"image_url\": image_url}]\n",
    "            }\n",
    "        ]\n",
    ")\n",
    "print(response_pic.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "689ce62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two cats.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "local_image_path = './resource/cat.jpg'  # Path to the local image file (corrected path)\n",
    "\n",
    "# Encode the image\n",
    "base64_image = encode_image(local_image_path)\n",
    "\n",
    "response_pic = client.chat.completions.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    # temperature and max_tokens are not supported for gpt-5-nano\n",
    "    # temperature=0.7, # not supported for gpt-5-nano\n",
    "    # max_tokens=300, # not supported for gpt-5-nano\n",
    "    top_p=1.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": [{\"type\": \"text\", \"text\": \"What is this a picture of?\"}, \n",
    "                     {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}]\n",
    "            }\n",
    "        ]\n",
    ")\n",
    "print(response_pic.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648eab63",
   "metadata": {},
   "source": [
    "### LLM Calls with Multiple Rounds\n",
    "This is useful for more complex interactions where the model needs to maintain context over multiple exchanges.\n",
    "\n",
    "Different from the single round call, we need to `maintain a list of messages` that represent the conversation history. The messages list is constructed with alternating user and assistant roles.\n",
    "\n",
    "The `messages` list looks like this:\n",
    "\n",
    "[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}, \n",
    "\n",
    "{\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "\n",
    "{\"role\": \"user\", \"content\": \"What is the population of Paris?\"},\n",
    "\n",
    "{\"role\": \"assistant\", \"content\": \"The population of Paris is approximately 2.1 million.\"}\n",
    "...\n",
    "]\n",
    "\n",
    "The messages represent the conversation history, allowing the model to understand the context of the current interaction. In some sense, it is a type of `Retrieval-Augmented Generation (RAG)` where the model retrieves relevant information from the `conversation history` to generate a response.\n",
    "\n",
    "#### Example of Multi-Round Chat\n",
    "\n",
    "Run the following code to see how to make a multi-round chat with the OpenAI API. Make sure to replace the `api_key` and `base_url` with your own values or set them in the `.env` file.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4bcafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load environment variables from .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") # Use correct env var name from your .env or manually set it\n",
    "print(\"OpenAI API Key:\", api_key[:10] + \"...\" if api_key else \"Not found\")  # Only show first 10 chars for security\n",
    "base_url = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")  # Use correct env var name from your .env\n",
    "print(\"OpenAI Base URL:\", base_url)\n",
    "\n",
    "# Initialize OpenAI client with API key and base URL\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_round_chat(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\", # or \"gpt-5-nano\" for the preview version\n",
    "        # temperature and max_tokens are not supported for gpt-5-nano\n",
    "        # temperature=0.7, # not supported for gpt-5-nano\n",
    "        # max_tokens=400, # not supported for gpt-5-nano\n",
    "        top_p=1.0,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content  \n",
    "\n",
    "user_input = input(\"You: \")\n",
    "print(\"You:\", user_input)\n",
    "# Initialize messages with the user's input\n",
    "messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "for _ in range(3):\n",
    "    response = multi_round_chat(messages)\n",
    "    print(\"Assistant:\")\n",
    "    display(Markdown(response)) \n",
    "    print(\"-\" * 50) \n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    user_input = input(\"You: \")\n",
    "    print(\"You:\", user_input)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9997bc",
   "metadata": {},
   "source": [
    "The code above allows for a multi-round conversation with the LLM. The user can input a message, and the assistant will respond based on the conversation history. The loop continues until the user decides to stop. It can be viewed as a simple chat interface with the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8cbad9",
   "metadata": {},
   "source": [
    "## A Simple Database: Dictionary or JSON\n",
    "A large quantity of data are stored in a structured format, such as JSON or XML. LLMs can also return structured data in the response.\n",
    "```python\n",
    "house_data = [\n",
    "    {\n",
    "        \"address\": \"123 Main St\",\n",
    "        \"price\": 500000,\n",
    "        \"bedrooms\": 3,\n",
    "        \"bathrooms\": 2,\n",
    "        \"features\": [\"garage\", \"garden\"]\n",
    "    },\n",
    "    {\n",
    "        \"address\": \"456 Elm St\",\n",
    "        \"price\": 600000,\n",
    "        \"bedrooms\": 4,\n",
    "        \"bathrooms\": 3,\n",
    "        \"features\": [\"pool\", \"fireplace\"]\n",
    "    }\n",
    "]\n",
    "```\n",
    "This is a simple example of how to structure the response from an LLM. With f-strings, you can easily format the output to include variable data in a readable way. For example:\n",
    "```python\n",
    "description = f\"The house at {house_data[0]['address']} has {house_data[0]['bedrooms']} bedrooms and is priced at ${house_data[0]['price']}. It features a {', '.join(house_data[0]['features'])}.\"\n",
    "\n",
    "print(description)\n",
    "```\n",
    "This will output:\n",
    "```\n",
    "The house at 123 Main St has 3 bedrooms and is priced at $500000. It features a garage, garden.\n",
    "```\n",
    "\n",
    "### Example of using a simple database\n",
    "Run the following code to see how to use a `simple database (a list of dictionaries in this case)` to store and retrieve information. This can be useful for applications like real estate listings, product catalogs, or any other structured data. The operation transforms the data into a readable format which can be `stored in a variable` and retrieved later. \n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc438176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data for a real estate listing\n",
    "house_data = [\n",
    "    {\"address\": \"123 Main St, Springfield\", \"price\": 500000, \"bedrooms\": 3, \"bathrooms\": 2, \"features\": [\"garage\", \"garden\"]},\n",
    "    {\"address\": \"456 Elm St, Springfield\", \"price\": 600000, \"bedrooms\": 4, \"bathrooms\": 3, \"features\": [\"pool\", \"fireplace\"]},\n",
    "    {\"address\": \"789 Oak St, Springfield\", \"price\": 450000, \"bedrooms\": 2, \"bathrooms\": 1, \"features\": [\"fenced yard\", \"new roof\"]},\n",
    "    {\"address\": \"101 Pine St, Springfield\", \"price\": 700000, \"bedrooms\": 5, \"bathrooms\": 4, \"features\": [\"home office\", \"basement\"]},\n",
    "    {\"address\": \"202 Maple St, Springfield\", \"price\": 550000, \"bedrooms\": 3, \"bathrooms\": 2, \"features\": [\"deck\", \"modern kitchen\"]},\n",
    "]\n",
    "# Accessing the data with simple print and formatting\n",
    "print(\"Real Estate Listings:\")\n",
    "for house in house_data:\n",
    "    print(f\"Address: {house['address']}, Price: ${house['price']}, Bedrooms: {house['bedrooms']}, Bathrooms: {house['bathrooms']}, Features: {', '.join(house['features'])}\")\n",
    "    print(\"-\" * 50)  # Separator for readability\n",
    "\n",
    "# Improved formatting with descriptive text\n",
    "def house_info(houses):\n",
    "    layout = ''\n",
    "    for house in houses:\n",
    "        layout += f\"House located at {house['address']} is priced at ${house['price']}. It has {house['bedrooms']} bedrooms and {house['bathrooms']} bathrooms. Notable features include: {', '.join(house['features'])}.\\n\"\n",
    "        layout += \"=*=\" * 20 + \"\\n\"  # Separator for readability\n",
    "    return layout\n",
    "print(\"Formatted Real Estate Listings:\")\n",
    "formatted_info = house_info(house_data)\n",
    "print(formatted_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f0475",
   "metadata": {},
   "source": [
    "## Putting It All Together (LLM + Structured Data)\n",
    "You can combine the LLM calls with structured data to create a more complex application. For example, you can use the LLM to generate a summary of a dataset or to answer questions about the data.\n",
    "\n",
    "The workflow can be summarized as follows:\n",
    "1. **Load the data**: Load the structured data from a file or database.\n",
    "2. **Process the data**: Use Python to process the data and prepare it for the\n",
    "3. **Call the LLM**: Use the processed data as input to the LLM, either as part of the prompt or as a separate message in a multi-round conversation.   \n",
    "\n",
    "`Diagram`\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Load Data] --> B[Process Data]        \n",
    "    B --> C[Call LLM]\n",
    "    C --> D[Receive Response]\n",
    "    D --> E[Display Result]\n",
    "    E --> F[User Input]\n",
    "    F --> C\n",
    "```\n",
    "### Simple Q&A Example with Structured Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing\n",
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") # Use correct env var name from your .env or manually set it\n",
    "print(\"OpenAI API Key:\", api_key[:10] + \"...\" if api_key else \"Not found\")  # Only show first 10 chars for security\n",
    "base_url = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")  # Use correct env var name from your .env\n",
    "print(\"OpenAI Base URL:\", base_url)\n",
    "\n",
    "# Initialize OpenAI client with API key and base URL\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585641c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "house_data = [\n",
    "    {\"address\": \"123 Main St, Springfield\", \"price\": 500000, \"bedrooms\": 3, \"bathrooms\": 2, \"features\": [\"garage\", \"garden\"]},\n",
    "    {\"address\": \"456 Elm St, Springfield\", \"price\": 600000, \"bedrooms\": 4, \"bathrooms\": 3, \"features\": [\"pool\", \"fireplace\"]},\n",
    "    {\"address\": \"789 Oak St, Springfield\", \"price\": 450000, \"bedrooms\": 2, \"bathrooms\": 1, \"features\": [\"fenced yard\", \"new roof\"]},\n",
    "    {\"address\": \"101 Pine St, Springfield\", \"price\": 700000, \"bedrooms\": 5, \"bathrooms\": 4, \"features\": [\"home office\", \"basement\"]},\n",
    "    {\"address\": \"202 Maple St, Springfield\", \"price\": 550000, \"bedrooms\": 3, \"bathrooms\": 2, \"features\": [\"deck\", \"modern kitchen\"]},\n",
    "]\n",
    "def house_info(houses):\n",
    "    layout = ''\n",
    "    for house in houses:\n",
    "        layout += f\"House located at {house['address']} is priced at ${house['price']}. It has {house['bedrooms']} bedrooms and {house['bathrooms']} bathrooms. Notable features include: {', '.join(house['features'])}.\\n\"\n",
    "        layout += \"=*=\" * 20 + \"\\n\"  # Separator for readability\n",
    "    return layout\n",
    "\n",
    "# Define a function to call LLM\n",
    "def sys_prompt(house_data, user_query):\n",
    "    prompt = \"You are a real estate assistant. Use the following houses information to answer users queries:\\n\"\n",
    "    prompt += house_info(house_data)\n",
    "    prompt += \"\\nNow, answer the user's query based on the provided house information. If you don't know the answer, say 'I don't know'. \\n\"\n",
    "    prompt += f\"User Query: {user_query}\\n\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_llm_response(prompt, api_key=api_key, base_url=base_url):\n",
    "        client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",  # or \"gpt-5-nano\" for the preview version\n",
    "        # temperature and max_tokens are not supported for gpt-5-nano\n",
    "        # temperature=0.7, # not supported for gpt-5-nano\n",
    "        # max_tokens=500, # not supported for gpt-5-nano\n",
    "        top_p=1.0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "        return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the conversation\n",
    "user_input = input(\"You: \")\n",
    "print(\"You:\", user_input)\n",
    "# Initialize messages with the user's input\n",
    "messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "# Generate response using the LLM with data\n",
    "response = generate_llm_response (sys_prompt(house_data, user_input), api_key=api_key, base_url=base_url)\n",
    "print(\"Assistant:\")\n",
    "display(Markdown(response))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea07cf0",
   "metadata": {},
   "source": [
    "### Muti-Round Conversation Example with Structured Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    user_input = input(\"You: \")\n",
    "    print(\"You:\", user_input)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    response = generate_llm_response(sys_prompt(house_data, user_input), api_key=api_key, base_url=base_url)\n",
    "    print(\"\\nAssistant:\")\n",
    "    display(Markdown(response)) \n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "\"\"\"The code above allows for a multi-round conversation with the LLM. \n",
    "The user can input a message, and the assistant will respond based on the conversation history. \n",
    "The loop continues until the user decides to stop. It can be viewed as a simple chat interface with the LLM.\"\"\"\n",
    "\n",
    "print(\"End of conversation. Thank you for using the assistant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eedc3c",
   "metadata": {},
   "source": [
    "### Example with RAG as an parameter\n",
    "In the following cell, we define a function that takes a prompt and an optional RAG parameter. If RAG is enabled, the function will format the prompt with the structured data before making the LLM call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7236c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_with_rag(prompt, api_key=api_key, base_url=base_url, use_RAG=True):\n",
    "\n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    if use_RAG:\n",
    "        prompt = sys_prompt(house_data, prompt)  # Use the sys_prompt function to format the prompt with RAG data\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",  # or \"gpt-5-nano\" for the preview version\n",
    "        # temperature and max_tokens are not supported for gpt-5-nano\n",
    "        # temperature=0.7, # not supported for gpt-5-nano\n",
    "        # max_tokens=500, # not supported for gpt-5-nano\n",
    "        top_p=1.0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "        return response.choices[0].message.content\n",
    "    else:\n",
    "        # If not using RAG, just return the prompt\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",  # or \"gpt-5-nano\" for the preview version\n",
    "        # temperature and max_tokens are not supported for gpt-5-nano\n",
    "        # temperature=0.7, # not supported for gpt-5-nano\n",
    "        # max_tokens=500, # not supported for gpt-5-nano\n",
    "        top_p=1.0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "        return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_input = input(\"You: \")\n",
    "print(\"You:\", user_input)\n",
    "# Initialize messages with the user's input\n",
    "response_generate_llm_with_rag = generate_llm_with_rag(user_input, api_key=api_key, base_url=base_url, use_RAG=True)\n",
    "print(\"RAG Assistant:\")\n",
    "display(Markdown(response_generate_llm_with_rag))\n",
    "\n",
    "user_input = input(\"You: \")\n",
    "print(\"You:\", user_input)\n",
    "# Generate response using the LLM without RAG\n",
    "response_generate_llm_with_rag_no_rag = generate_llm_with_rag(user_input, api_key=api_key, base_url=base_url, use_RAG=False)\n",
    "print(\"Assistant (without RAG):\")\n",
    "display(Markdown(response_generate_llm_with_rag_no_rag))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025bac6",
   "metadata": {},
   "source": [
    "========END========="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
