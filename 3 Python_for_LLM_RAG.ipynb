{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33bd405a",
   "metadata": {},
   "source": [
    "# Basic LLM Calls and RAG Workflow\n",
    "## Quick Start Guide for Large Language Models (LLMs)\n",
    "\n",
    "For practice purposes, I use OpenAI's GPT-4o-mini models for text based Q&A.  In this exercise, I will show how to make basic LLM calls and implement a Retrieval-Augmented Generation (RAG) workflow using Python. The \"database\" used in this example is a simple structured dummy dataset, which can be easily replaced with any other data source (Notebook 5).\n",
    "\n",
    "All environment variables are set in the `.env` file, which is not included in this repository for security reasons. A brief description of the `.env` file is provided below.\n",
    "\n",
    "The basic LLM call is made using the `openai` Python package, which is installed via pip. \n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "client = openai.Client(api_key=\"your-api-key\",base_url= 'your-base-url')  # Set your OpenAI API key and base URL\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o-mini\", # or \"gpt-4o-mini-preview\" for the preview version\n",
    "    temperature=0.7, # controls randomness in the response\n",
    "    max_tokens=100, # maximum number of tokens in the response\n",
    "    top_p=1.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    ")\n",
    "message = \"What is the capital of France?\"\n",
    "messages = \"You are a helpful assistant. Please answer the question: \" + message\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "### Getting started with OpenAI Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional\n",
    "!pip install openai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ff4f3",
   "metadata": {},
   "source": [
    "### Set up the OpenAI client\n",
    "\n",
    "With the latest OpenAI Python SDK, the way to call API has changed compared to older versions. The key difference is:\n",
    "\n",
    "- You must create a client instance (OpenAI()) to call the API.\n",
    "\n",
    "- The client instance will automatically read API key and base URL from environment variables if you donâ€™t pass them explicitly.\n",
    "\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"your-api-key\", api_base=\"your-base-url\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd6279d",
   "metadata": {},
   "source": [
    "### Getting Environment Variables from the .env File\n",
    "It is a good practice to store sensitive information like API keys in environment variables. You can use the `dotenv` or os package to load these variables from a `.env` file.\n",
    "\n",
    "You should avoid hardcoding sensitive information in your code. Instead, use environment variables to store them securely.\n",
    "\n",
    "A typical `.env` file might look like this:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your-api-key\n",
    "OPENAI_API_BASE_URL=your-base-url\n",
    "other_variable=value\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027df6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: sk-nzH4YCa...\n",
      "OpenAI Base URL: https://xiaoai.plus/v1\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Set API key and base URL globally\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") # Use correct env var name from your .env or manually set it\n",
    "print(\"OpenAI API Key:\", api_key[:10] + \"...\" if api_key else \"Not found\")  # Only show first 10 chars for security\n",
    "base_url = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")  # Use correct env var name from your .env\n",
    "print(\"OpenAI Base URL:\", base_url)\n",
    "\n",
    "# Initialize OpenAI client with API key and base URL\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3608dc6",
   "metadata": {},
   "source": [
    "### Calling the OpenAI API\n",
    "To get a response from the OpenAI API, you can use the `client.chat.completions.create` method. This method takes several parameters, including the model to use, the temperature, and the messages to send to the model.\n",
    "\n",
    "The basic syntax for calling the OpenAI API is as follows:\n",
    "\n",
    "Step 1: Import the necessary libraries and set up the OpenAI client.\n",
    "\n",
    "Step 2: Create a client instance with your API key and base URL.\n",
    "\n",
    "`Step 3: Define a function to generate a response from the LLM, optionally using RAG data.`\n",
    "\n",
    "There are several critical parameters to consider when making an LLM call:\n",
    "- `model`: The model to use for the LLM call, such as \"gpt-4o-mini\" or \"gpt-4o-mini-preview\". This determines the capabilities, performance and cost of the LLM.\n",
    "- `max_tokens`: The maximum number of tokens to generate in the response. This limits the length of the response and helps control costs.\n",
    "- `top_p`: Controls the diversity of the response. A value of 1.0 means no filtering, while lower values (e.g., 0.9) restrict the response to more probable tokens.\n",
    "- `messages`: A list of messages to send to the model. Each message is a dictionary with a `role` (either \"user\" or \"assistant\") and `content` (the text of the message). This allows for multi-turn conversations and context management.\n",
    "- `temperature`: Controls the randomness of the response. A higher temperature (e.g., 0.7) results in more creative responses, while a lower temperature (e.g., 0.2) results in more focused and deterministic responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d7bd0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the OpenAI client\n",
    "message = \"What is the capital of France?\"\n",
    "\n",
    "messages = \"You are a helpful assistant. Please answer the question: \" + message \n",
    "#this is the final prompt sent\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", # or \"gpt-4o-mini-preview\" for the preview version\n",
    "    temperature=0.7, # controls randomness in the response\n",
    "    max_tokens=100, # maximum number of tokens in the response\n",
    "    top_p=1.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "# this will return the response from the model, which should be \"Paris\" for this question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcae0b92",
   "metadata": {},
   "source": [
    "LLMs can answer quetions about non-text data, such as images, audio, and video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d839b0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a picture of an adorable kitten with its mouth open, looking excited or playful, inside or near a cardboard box.\n"
     ]
    }
   ],
   "source": [
    "# Answer question from picture with 4o and image\n",
    "image_url ='https://static.independent.co.uk/s3fs-public/thumbnails/image/2017/03/28/13/kitten.jpg?quality=75&width=1250&crop=3%3A2%2Csmart&auto=webp'\n",
    "\n",
    "response_pic = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    "    top_p=1.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": [{\"type\": \"text\", \"text\": \"What is this a picture of?\"}, \n",
    "                     {\"type\": \"image_url\", \"image_url\": image_url}]\n",
    "            }\n",
    "        ]\n",
    ")\n",
    "print(response_pic.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "689ce62e",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'error': {'message': 'failed to decode base64 string: illegal base64 data at input byte 0 (request id: 20250730145235994276609nS2sEKTl)', 'type': 'new_api_error', 'param': '', 'code': 'count_token_messages_failed'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m local_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./resources/cat.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Path to the local image file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m response_pic \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m      6\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m      7\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m      8\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      9\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     10\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is this a picture of?\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m     11\u001b[0m                      {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: local_image_path}]\n\u001b[1;32m     12\u001b[0m             }\n\u001b[1;32m     13\u001b[0m         ]\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_pic\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:579\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    578\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    581\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    582\u001b[0m             {\n\u001b[1;32m    583\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    584\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    585\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    586\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    587\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    589\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    590\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    591\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    592\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    593\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    595\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    596\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    597\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    598\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    599\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    600\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    601\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    602\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    603\u001b[0m             },\n\u001b[1;32m    604\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    605\u001b[0m         ),\n\u001b[1;32m    606\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    607\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    608\u001b[0m         ),\n\u001b[1;32m    609\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    610\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    611\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    612\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1232\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1220\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1228\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1229\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1230\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1231\u001b[0m     )\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    922\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    923\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    924\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    925\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    926\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    927\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:959\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    956\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    960\u001b[0m         options,\n\u001b[1;32m    961\u001b[0m         cast_to,\n\u001b[1;32m    962\u001b[0m         retries,\n\u001b[1;32m    963\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    964\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    965\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1045\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1046\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1047\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1048\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1049\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1050\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1051\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:997\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    996\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    998\u001b[0m         options,\n\u001b[1;32m    999\u001b[0m         cast_to,\n\u001b[1;32m   1000\u001b[0m         retries,\n\u001b[1;32m   1001\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1002\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1003\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1004\u001b[0m     )\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1045\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1046\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1047\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1048\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1049\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1050\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1051\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1012\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1011\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1012\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1015\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1016\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1020\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 500 - {'error': {'message': 'failed to decode base64 string: illegal base64 data at input byte 0 (request id: 20250730145235994276609nS2sEKTl)', 'type': 'new_api_error', 'param': '', 'code': 'count_token_messages_failed'}}"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "local_image_path = './resource/cat.jpg'  # Path to the local image file (corrected path)\n",
    "\n",
    "# Encode the image\n",
    "base64_image = encode_image(local_image_path)\n",
    "\n",
    "response_pic = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    "    top_p=1.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": [{\"type\": \"text\", \"text\": \"What is this a picture of?\"}, \n",
    "                     {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}]\n",
    "            }\n",
    "        ]\n",
    ")\n",
    "print(response_pic.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648eab63",
   "metadata": {},
   "source": [
    "### LLM Calls with Multiple Rounds\n",
    "This is useful for more complex interactions where the model needs to maintain context over multiple exchanges.\n",
    "\n",
    "Different from the single round call, we need to `maintain a list of messages` that represent the conversation history. The messages list is constructed with alternating user and assistant roles.\n",
    "\n",
    "The `messages` list looks like this:\n",
    "\n",
    "[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}, \n",
    "\n",
    "{\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "\n",
    "{\"role\": \"user\", \"content\": \"What is the population of Paris?\"},\n",
    "\n",
    "{\"role\": \"assistant\", \"content\": \"The population of Paris is approximately 2.1 million.\"}\n",
    "...\n",
    "]\n",
    "\n",
    "The messages represent the conversation history, allowing the model to understand the context of the current interaction. In some sense, it is a type of `Retrieval-Augmented Generation (RAG)` where the model retrieves relevant information from the `conversation history` to generate a response.\n",
    "\n",
    "#### Example of Multi-Round Chat\n",
    "\n",
    "Run the following code to see how to make a multi-round chat with the OpenAI API. Make sure to replace the `api_key` and `base_url` with your own values or set them in the `.env` file.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4bcafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load environment variables from .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") # Use correct env var name from your .env or manually set it\n",
    "print(\"OpenAI API Key:\", api_key[:10] + \"...\" if api_key else \"Not found\")  # Only show first 10 chars for security\n",
    "base_url = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")  # Use correct env var name from your .env\n",
    "print(\"OpenAI Base URL:\", base_url)\n",
    "\n",
    "# Initialize OpenAI client with API key and base URL\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_round_chat(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", # or \"gpt-4o-mini-preview\" for the preview version\n",
    "        temperature=0.7, # controls randomness in the response\n",
    "        max_tokens=400, # maximum number of tokens in the response\n",
    "        top_p=1.0,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content  \n",
    "\n",
    "user_input = input(\"You: \")\n",
    "print(\"You:\", user_input)\n",
    "# Initialize messages with the user's input\n",
    "messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "for _ in range(3):\n",
    "    response = multi_round_chat(messages)\n",
    "    print(\"Assistant:\")\n",
    "    display(Markdown(response)) \n",
    "    print(\"-\" * 50) \n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    user_input = input(\"You: \")\n",
    "    print(\"You:\", user_input)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9997bc",
   "metadata": {},
   "source": [
    "The code above allows for a multi-round conversation with the LLM. The user can input a message, and the assistant will respond based on the conversation history. The loop continues until the user decides to stop. It can be viewed as a simple chat interface with the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8cbad9",
   "metadata": {},
   "source": [
    "## A Simple Database: Dictionary or JSON\n",
    "A large quantity of data are stored in a structured format, such as JSON or XML. LLMs can also return structured data in the response.\n",
    "```python\n",
    "house_data = [\n",
    "    {\n",
    "        \"address\": \"123 Main St\",\n",
    "        \"price\": 500000,\n",
    "        \"bedrooms\": 3,\n",
    "        \"bathrooms\": 2,\n",
    "        \"features\": [\"garage\", \"garden\"]\n",
    "    },\n",
    "    {\n",
    "        \"address\": \"456 Elm St\",\n",
    "        \"price\": 600000,\n",
    "        \"bedrooms\": 4,\n",
    "        \"bathrooms\": 3,\n",
    "        \"features\": [\"pool\", \"fireplace\"]\n",
    "    }\n",
    "]\n",
    "```\n",
    "This is a simple example of how to structure the response from an LLM. With f-strings, you can easily format the output to include variable data in a readable way. For example:\n",
    "```python\n",
    "description = f\"The house at {house_data[0]['address']} has {house_data[0]['bedrooms']} bedrooms and is priced at ${house_data[0]['price']}. It features a {', '.join(house_data[0]['features'])}.\"\n",
    "\n",
    "print(description)\n",
    "```\n",
    "This will output:\n",
    "```\n",
    "The house at 123 Main St has 3 bedrooms and is priced at $500000. It features a garage, garden.\n",
    "```\n",
    "\n",
    "### Example of using a simple database\n",
    "Run the following code to see how to use a `simple database (a list of dictionaries in this case)` to store and retrieve information. This can be useful for applications like real estate listings, product catalogs, or any other structured data. The operation transforms the data into a readable format which can be `stored in a variable` and retrieved later. \n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc438176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data for a real estate listing\n",
    "house_data = [\n",
    "    {\"address\": \"123 Main St, Springfield\", \"price\": 500000, \"bedrooms\": 3, \"bathrooms\": 2, \"features\": [\"garage\", \"garden\"]},\n",
    "    {\"address\": \"456 Elm St, Springfield\", \"price\": 600000, \"bedrooms\": 4, \"bathrooms\": 3, \"features\": [\"pool\", \"fireplace\"]},\n",
    "    {\"address\": \"789 Oak St, Springfield\", \"price\": 450000, \"bedrooms\": 2, \"bathrooms\": 1, \"features\": [\"fenced yard\", \"new roof\"]},\n",
    "    {\"address\": \"101 Pine St, Springfield\", \"price\": 700000, \"bedrooms\": 5, \"bathrooms\": 4, \"features\": [\"home office\", \"basement\"]},\n",
    "    {\"address\": \"202 Maple St, Springfield\", \"price\": 550000, \"bedrooms\": 3, \"bathrooms\": 2, \"features\": [\"deck\", \"modern kitchen\"]},\n",
    "]\n",
    "# Accessing the data with simple print and formatting\n",
    "print(\"Real Estate Listings:\")\n",
    "for house in house_data:\n",
    "    print(f\"Address: {house['address']}, Price: ${house['price']}, Bedrooms: {house['bedrooms']}, Bathrooms: {house['bathrooms']}, Features: {', '.join(house['features'])}\")\n",
    "    print(\"-\" * 50)  # Separator for readability\n",
    "\n",
    "# Improved formatting with descriptive text\n",
    "def house_info(houses):\n",
    "    layout = ''\n",
    "    for house in houses:\n",
    "        layout += f\"House located at {house['address']} is priced at ${house['price']}. It has {house['bedrooms']} bedrooms and {house['bathrooms']} bathrooms. Notable features include: {', '.join(house['features'])}.\\n\"\n",
    "        layout += \"=*=\" * 20 + \"\\n\"  # Separator for readability\n",
    "    return layout\n",
    "print(\"Formatted Real Estate Listings:\")\n",
    "formatted_info = house_info(house_data)\n",
    "print(formatted_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f0475",
   "metadata": {},
   "source": [
    "## Putting It All Together (LLM + Structured Data)\n",
    "You can combine the LLM calls with structured data to create a more complex application. For example, you can use the LLM to generate a summary of a dataset or to answer questions about the data.\n",
    "\n",
    "The workflow can be summarized as follows:\n",
    "1. **Load the data**: Load the structured data from a file or database.\n",
    "2. **Process the data**: Use Python to process the data and prepare it for the\n",
    "3. **Call the LLM**: Use the processed data as input to the LLM, either as part of the prompt or as a separate message in a multi-round conversation.   \n",
    "\n",
    "`Diagram`\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Load Data] --> B[Process Data]        \n",
    "    B --> C[Call LLM]\n",
    "    C --> D[Receive Response]\n",
    "    D --> E[Display Result]\n",
    "    E --> F[User Input]\n",
    "    F --> C\n",
    "```\n",
    "### Simple Q&A Example with Structured Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2709cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: sk-nzH4YCa...\n",
      "OpenAI Base URL: https://xiaoai.plus/v1\n"
     ]
    }
   ],
   "source": [
    "# Initializing\n",
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") # Use correct env var name from your .env or manually set it\n",
    "print(\"OpenAI API Key:\", api_key[:10] + \"...\" if api_key else \"Not found\")  # Only show first 10 chars for security\n",
    "base_url = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")  # Use correct env var name from your .env\n",
    "print(\"OpenAI Base URL:\", base_url)\n",
    "\n",
    "# Initialize OpenAI client with API key and base URL\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585641c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "house_data = [\n",
    "    {\"address\": \"123 Main St, Springfield\", \"price\": 500000, \"bedrooms\": 3, \"bathrooms\": 2, \"features\": [\"garage\", \"garden\"]},\n",
    "    {\"address\": \"456 Elm St, Springfield\", \"price\": 600000, \"bedrooms\": 4, \"bathrooms\": 3, \"features\": [\"pool\", \"fireplace\"]},\n",
    "    {\"address\": \"789 Oak St, Springfield\", \"price\": 450000, \"bedrooms\": 2, \"bathrooms\": 1, \"features\": [\"fenced yard\", \"new roof\"]},\n",
    "    {\"address\": \"101 Pine St, Springfield\", \"price\": 700000, \"bedrooms\": 5, \"bathrooms\": 4, \"features\": [\"home office\", \"basement\"]},\n",
    "    {\"address\": \"202 Maple St, Springfield\", \"price\": 550000, \"bedrooms\": 3, \"bathrooms\": 2, \"features\": [\"deck\", \"modern kitchen\"]},\n",
    "]\n",
    "def house_info(houses):\n",
    "    layout = ''\n",
    "    for house in houses:\n",
    "        layout += f\"House located at {house['address']} is priced at ${house['price']}. It has {house['bedrooms']} bedrooms and {house['bathrooms']} bathrooms. Notable features include: {', '.join(house['features'])}.\\n\"\n",
    "        layout += \"=*=\" * 20 + \"\\n\"  # Separator for readability\n",
    "    return layout\n",
    "\n",
    "# Define a function to call LLM\n",
    "def sys_prompt(house_data, user_query):\n",
    "    prompt = \"You are a real estate assistant. Use the following houses information to answer users queries:\\n\"\n",
    "    prompt += house_info(house_data)\n",
    "    prompt += \"\\nNow, answer the user's query based on the provided house information. If you don't know the answer, say 'I don't know'. \\n\"\n",
    "    prompt += f\"User Query: {user_query}\\n\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_llm_response(prompt, api_key=api_key, base_url=base_url):\n",
    "        client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4o-mini-preview\" for the preview version\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "        top_p=1.0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "        return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d89fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: \n",
      "Assistant:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start the conversation\n",
    "user_input = input(\"You: \")\n",
    "print(\"You:\", user_input)\n",
    "# Initialize messages with the user's input\n",
    "messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "# Generate response using the LLM with data\n",
    "response = generate_llm_response (sys_prompt(house_data, user_input), api_key=api_key, base_url=base_url)\n",
    "print(\"Assistant:\")\n",
    "display(Markdown(response))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea07cf0",
   "metadata": {},
   "source": [
    "### Muti-Round Conversation Example with Structured Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: smallest house?\n",
      "Assistant:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The smallest house is located at 789 Oak St, Springfield. It has 2 bedrooms and 1 bathroom, and is priced at $450,000. Notable features include a fenced yard and a new roof."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    user_input = input(\"You: \")\n",
    "    print(\"You:\", user_input)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    response = generate_llm_response(sys_prompt(house_data, user_input), api_key=api_key, base_url=base_url)\n",
    "    print(\"\\nAssistant:\")\n",
    "    display(Markdown(response)) \n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "\"\"\"The code above allows for a multi-round conversation with the LLM. \n",
    "The user can input a message, and the assistant will respond based on the conversation history. \n",
    "The loop continues until the user decides to stop. It can be viewed as a simple chat interface with the LLM.\"\"\"\n",
    "\n",
    "print(\"End of conversation. Thank you for using the assistant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eedc3c",
   "metadata": {},
   "source": [
    "### Example with RAG as an parameter\n",
    "In the following cell, we define a function that takes a prompt and an optional RAG parameter. If RAG is enabled, the function will format the prompt with the structured data before making the LLM call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7236c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_with_rag(prompt, api_key=api_key, base_url=base_url, use_RAG=True):\n",
    "\n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    if use_RAG:\n",
    "        prompt = sys_prompt(house_data, prompt)  # Use the sys_prompt function to format the prompt with RAG data\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4o-mini-preview\" for the preview version\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "        top_p=1.0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "        return response.choices[0].message.content\n",
    "    else:\n",
    "        # If not using RAG, just return the prompt\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4o-mini-preview\" for the preview version\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "        top_p=1.0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "        return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d8d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: smallest house\n",
      "Assistant:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The smallest house is located at 789 Oak St, Springfield. It has 2 bedrooms and 1 bathroom, and it is priced at $450,000. Notable features include a fenced yard and a new roof."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: smallest house\n",
      "Assistant (without RAG):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The title of the \"smallest house\" can vary depending on criteria, but one of the most famous examples is the \"Tiny House\" movement, which promotes living in extremely small, efficient homes. One of the smallest houses recognized is the \"One-Sqm House\" designed by studio 1:1 Architects in Poland, which is just 1 square meter (about 10.76 square feet) in size. \n",
       "\n",
       "In more general terms, there are many tiny houses and micro-apartments around the world, often ranging from 100 to 400 square feet, designed to maximize space and minimize living costs. The concept encourages simplicity and sustainability.\n",
       "\n",
       "If you're interested in a specific type or style of small house, let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "user_input = input(\"You: \")\n",
    "print(\"You:\", user_input)\n",
    "# Initialize messages with the user's input\n",
    "response_generate_llm_with_rag = generate_llm_with_rag(user_input, api_key=api_key, base_url=base_url, use_RAG=True)\n",
    "print(\"RAG Assistant:\")\n",
    "display(Markdown(response_generate_llm_with_rag))\n",
    "\n",
    "user_input = input(\"You: \")\n",
    "print(\"You:\", user_input)\n",
    "# Generate response using the LLM without RAG\n",
    "response_generate_llm_with_rag_no_rag = generate_llm_with_rag(user_input, api_key=api_key, base_url=base_url, use_RAG=False)\n",
    "print(\"Assistant (without RAG):\")\n",
    "display(Markdown(response_generate_llm_with_rag_no_rag))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
