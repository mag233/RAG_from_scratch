{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9897f031",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "\n",
    "Traditionally, information retrieval (IR) has been a key area in computer science, focusing on finding relevant documents from large collections based on user queries. The most common and time-tested method is `keyword search`, where documents are indexed based on the words they contain. When a user inputs a query, the system retrieves documents that match the keywords in the query.\n",
    "\n",
    "In the age of large language models (LLMs), the landscape of IR is evolving. LLMs can understand and generate human-like text, allowing for more sophisticated interactions. Instead of just matching keywords, LLMs can comprehend the `context and semantics` of a query, leading to more relevant and nuanced results.\n",
    "\n",
    "## Search Techniques\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a3912",
   "metadata": {},
   "source": [
    "![Search Techniques](./resource/search_technique.png)\n",
    "\n",
    "*Screenshot from the course showing different search techniques including keyword search and semantic search with metadata filtering*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3e7da",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "TF-IDF, or Term Frequency-Inverse Document Frequency, is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It combines two components:\n",
    "\n",
    "- `Term Frequency (TF)`: Measures how frequently a term appears in a document. The more a term appears, the more relevant it is considered to be for that document.\n",
    "- \n",
    "- `Inverse Document Frequency (IDF)`: Measures how important a term is across the entire corpus. A term that appears in many documents is less informative than a term that appears in only a few. IDF is calculated as the logarithm of the total number of documents divided by the number of documents containing the term.\n",
    "- \n",
    "- The final TF-IDF score is the product of these two components, providing a balanced measure of a term's relevance in a specific document relative to the entire corpus.\n",
    "\n",
    "### How to calculate TF-IDF:\n",
    "```math\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "```\n",
    "Where:\n",
    "- `t` is the term,\n",
    "\n",
    "- `d` is the document,\n",
    "\n",
    "- `TF(t, d)` is the term frequency of term `t` in document `d`, calculated as:\n",
    "```math\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
    "```\n",
    "- `IDF(t)` is the inverse document frequency of term `t`, calculated as:\n",
    "```math\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents in corpus}}{\\text{Number of documents containing term } t}\\right)\n",
    "```\n",
    "\n",
    "- The TF-IDF score is higher for terms that are frequent in a document but rare across the corpus, making it a powerful tool for identifying relevant documents in information retrieval tasks.\n",
    "\n",
    "For example, if we have a document containing the term \"AI\" 5 times out of 100 total terms, and \"AI\" appears in 10 out of 1000 documents in the corpus, the TF-IDF score for \"AI\" in that document would be calculated as follows:\n",
    "```math\n",
    "\\text{TF-IDF}(\\text{\"AI\"}, d) = \\left(\\frac{5}{100}\\right) \\times \\log\\left(\\frac{1000}{10}\\right) = 0.05 \\times \\log(100) = 0.05 \\times 2 = 0.1\n",
    "```\n",
    "In comparison, if another term \"ML\" appears 2 times in the same document and is present in 100 out of 1000 documents, its TF-IDF score would be:\n",
    "```math\n",
    "\\text{TF-IDF}(\\text{\"ML\"}, d) = \\left(\\frac{2}{100}\\right) \\times \\log\\left(\\frac{1000}{100}\\right) = 0.02 \\times \\log(10) = 0.02 \\times 1 = 0.02\n",
    "```\n",
    "This shows that \"AI\" is more relevant to the document than \"ML\" based on the TF-IDF scores, highlighting its importance in the context of the document and the corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5210cbd",
   "metadata": {},
   "source": [
    "## BM25\n",
    "BM25, or Best Matching 25, is an advanced ranking function used in information retrieval that builds upon the TF-IDF model. It incorporates several enhancements to improve the relevance of search results:\n",
    "- `Term Frequency Saturation`: Unlike TF-IDF, BM25 applies a saturation effect to term frequency, meaning that the relevance of a term increases with its frequency but at a diminishing rate. This prevents overly frequent terms from disproportionately influencing the score.\n",
    "\n",
    "- `Document Length Normalization`: BM25 normalizes the term frequency by considering the length of the document. This helps to ensure that longer documents do not unfairly receive higher scores simply due to their length.\n",
    "\n",
    "- `IDF Component`: BM25 uses a modified version of the IDF component, which is more robust against terms that appear in many documents. It adjusts the IDF calculation to better reflect the rarity of a term across the corpus.\n",
    "\n",
    "### Advantages of BM25:\n",
    "\n",
    "- `Improved Relevance`: By combining term frequency saturation and document length normalization, BM25 provides more accurate relevance scores compared to traditional TF-IDF.\n",
    "\n",
    "- `Flexibility`: BM25 allows for tuning parameters (`k_1` and `b`) that can be adjusted based on the specific characteristics of the corpus, making it adaptable to different types of documents and queries.\n",
    "\n",
    "- `Widely Used`: BM25 is a standard in information retrieval and is implemented in many search engines and libraries, making it a reliable choice for building search systems.\n",
    "\n",
    "\n",
    "![BM25](./resource/BM25.jpg)\n",
    "*Screenshot from the course showing the BM25 formula and its components*\n",
    "\n",
    "### How to calculate BM25:\n",
    "```math\n",
    "\\text{BM25}(t, d) = \\text{IDF}(t)\\times \\frac{\\text{TF}(t, d) \\times (k_1 + 1)}{\\text{TF}(t, d) + k_1 \\times (1 - b + b \\times \\frac{\\text{Length}(d)}{\\text{Avg.Length}})}\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `t` is the term,\n",
    "\n",
    "- `d` is the document,\n",
    "\n",
    "- `TF(t, d)` is the term frequency of term `t` in document `d`,\n",
    "\n",
    "- `IDF(t)` is the inverse document frequency of term `t`, calculated as:\n",
    "```math\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents in corpus} - \\text{Number of documents containing term } t + 0.5}{\\text{Number of documents containing term } t + 0.5}\\right)\n",
    "```\n",
    "- `k_1` and `b` are tuning parameters that control the term frequency saturation and document length normalization, respectively. Common values are `k_1 = 1.2` and `b = 0.75`.\n",
    "\n",
    "\n",
    "BM25 is particularly effective in scenarios where the relevance of documents needs to be assessed based on both the frequency of terms and the overall structure of the documents. It is widely used in search engines and information retrieval systems due to its ability to provide more accurate and relevant results compared to traditional TF-IDF methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07cfca0",
   "metadata": {},
   "source": [
    "### Python Implementation of BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed47db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Obtaining dependency information for rank_bm25 from https://files.pythonhosted.org/packages/2a/21/f691fb2613100a62b3fa91e9988c991e9ca5b89ea31c0d3152a3210344f9/rank_bm25-0.2.2-py3-none-any.whl.metadata\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: nltk in /Users/maggiezhao/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/maggiezhao/anaconda3/lib/python3.11/site-packages (from rank_bm25) (1.24.3)\n",
      "Requirement already satisfied: click in /Users/maggiezhao/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/maggiezhao/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/maggiezhao/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/maggiezhao/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcfa96f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rank_bm25'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrank_bm25\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25Okapi\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rank_bm25'"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import download\n",
    "download('punkt') #punkt is used for tokenization\n",
    "download('stopwords') #stopwords are used for filtering out common words\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"Cats and dogs are great pets.\",\n",
    "    \"Dogs are loyal and friendly.\",\n",
    "    \"Cats are independent and curious.\"\n",
    "]\n",
    "# Preprocess documents: tokenize and remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_docs = []\n",
    "for doc in documents:       \n",
    "    tokens = word_tokenize(doc.lower())  # Tokenize and convert to lowercase\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]  # Remove stopwords and punctuation\n",
    "    tokenized_docs.append(tokens)\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "# Sample query\n",
    "query = \"cat and dog\"\n",
    "# Preprocess query\n",
    "query_tokens = word_tokenize(query.lower())\n",
    "query_tokens = [word for word in query_tokens if word not in stop_words and word not in string.punctuation]\n",
    "# Get BM25 scores for the query\n",
    "scores = bm25.get_scores(query_tokens)\n",
    "# Print scores\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"Document {i+1}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed061d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
