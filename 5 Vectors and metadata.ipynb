{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4977d4c",
   "metadata": {},
   "source": [
    "# Vector Database and the Use of Metadata\n",
    "\n",
    "## Framework of RAG\n",
    "\n",
    "1. **Create a vector store:** This involves embedding documents into a vector space using a model like Sentence Transformers or OpenAI's text-embedding-ada-002.\n",
    "\n",
    "2. **Add documents to the vector store:** This step involves storing the embedded documents in a vector database such as Pinecone, Weaviate, or a local vector store like FAISS.\n",
    "\n",
    "3. **Query the vector store by calculating similarity between the query and the documents:** This is done by embedding the query and comparing it to the stored document vectors using cosine similarity or other distance metrics.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./resource/vec.jpg\" alt=\"Vector Search\" width=\"800\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a8ea1",
   "metadata": {},
   "source": [
    "## Vectors: word embeddings vs. document embeddings\n",
    "\n",
    "Vectors are numerical representations of words or documents in a high-dimensional space. They capture semantic meaning, allowing for similarity comparisons. But there are two types of vectors that can easily confuse beginners:\n",
    "\n",
    "1. `Word embeddings:` These are vectors representing individual words, capturing their meanings and relationships. They are typically generated using models like Word2Vec or GloVe.\n",
    "\n",
    "2. `Document embeddings:` These vectors represent entire documents, capturing the overall meaning and context. They are often generated by aggregating word embeddings or using models like Sentence Transformers or OpenAI's text-embedding-ada-002.\n",
    "\n",
    "For RAG systems, we primarily use document embeddings, as they allow us to compare the meaning of entire documents rather than just individual words. Note that the `\"document\"` in this context can be a sentence, paragraph, or even an entire article, depending on the granularity of the information we want to retrieve. It is NOT limited to what we usually think of as a \"document\" in the traditional sense. This confused the hell out of me when I first learned about RAG systems, so I want to clarify it here.\n",
    "\n",
    "There are many models available for generating document embeddings, each with its own strengths and weaknesses. \n",
    "\n",
    "Table 5.1 Popular models for document embeddings\n",
    "| Model Name | Description | Use Case |Local Option | Commercial Option |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| OpenAI's text-embedding-ada-002 | A powerful model for generating document embeddings. | General-purpose document embedding. | No | Yes |\n",
    "| BAAI/bge-base-en-v1.5 | A model designed for generating document embeddings. | General-purpose document embedding. | Yes | No |\n",
    "| intfloat/e5-base | A model designed for generating document embeddings. | General-purpose document embedding. | Yes | No |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea142c9",
   "metadata": {},
   "source": [
    "I will use sentence-transformers' `BAAI/bge-base-en-v1.5` model to generate document embeddings in this notebook. This model is designed for generating document embeddings and is available for local use, making it a good choice for many applications. It will convert text into a 768-dimensional vector in numpy ndarray format, which is a common size for document embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110dfff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y keras keras-nightly keras-preprocessing keras-vis\n",
    "!pip uninstall -y tf-keras-nightly tf-keras\n",
    "!pip install tf-keras --upgrade\n",
    "!pip install --upgrade transformers sentence-transformers\n",
    "# install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('BAAI/bge-base-en-v1.5') \n",
    "# assign the model to a variable, if the model is not downloaded, it will be downloaded automatically\n",
    "# so the model is running locally, it's not a big model and can be run on a laptop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"query: What are the symptoms of COVID-19?\"]\n",
    "embedding = model.encode(query, normalize_embeddings=True)\n",
    "print(embedding.shape)  # Output: (1, 768), which means the query is represented by a 768-dimensional vector\n",
    "print(embedding[:, :5])  # first 5 elements of the embedding\n",
    "# model.encode(str) returns a numpy array which represents the embedding of the input text\n",
    "# the bge-base-en-v1.5 model produces 768-dimensional embeddings ((768,) means a 1D array with 768 elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78699973",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.encode(\"RAG is awesome\") \n",
    "#model.encode(str) returns a numpy array which represents the embedding of the input text\n",
    "print(res.shape) # Output: (768,) \n",
    "# the bge-base-en-v1.5 model produces 768-dimensional embeddings ((768,) means a 1D array with 768 elements)\n",
    "print(res[:5]) # first 5 elements of the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d247ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of encoding multiple words\n",
    "# We can pass a list of words to the model to get their embeddings\n",
    "\n",
    "words = ['apple', 'car', 'fruit', 'automobile', 'love', 'sentiment','book']\n",
    "vectorized_words = model.encode(words) \n",
    "# returns a 2D numpy array where each row is the embedding of a word\n",
    "\n",
    "print(vectorized_words.shape) \n",
    "# Output: (7, 768), which means there are 7 words, each represented by a 768-dimensional vector\n",
    "\n",
    "print(vectorized_words[:5,:5]) # first 5 word embeddings' first 5 elements\n",
    "# there's a note on numpy slicing at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dbd1c1",
   "metadata": {},
   "source": [
    "[→Jump to numpy slicing reference←](#quick-refresher-note-on-slicing-numpy-arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity Function\n",
    "def cosine_similarity(v1, array_of_vectors):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between a vector and an array of vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    v1 (array-like): The first vector.\n",
    "    array_of_vectors (array-like): An array of vectors or a single vector.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of cosine similarities between v1 and each vector in array_of_vectors.\n",
    "    \"\"\"\n",
    "    # Ensure that v1 is a numpy array\n",
    "    v1 = np.array(v1)\n",
    "    # Initialize a list to store similarities\n",
    "    similarities = []\n",
    "    \n",
    "    # Check if array_of_vectors is a single vector\n",
    "    if len(np.shape(array_of_vectors)) == 1:\n",
    "        array_of_vectors = [array_of_vectors]\n",
    "    \n",
    "    # Iterate over each vector in the array\n",
    "    for v2 in array_of_vectors:\n",
    "        # Convert the current vector to a numpy array\n",
    "        v2 = np.array(v2)\n",
    "        # Compute the dot product of v1 and v2\n",
    "        dot_product = np.dot(v1, v2)\n",
    "        # Compute the norms of the vectors, np.linalg.norm(vector) computes the length of the vector\n",
    "        norm_v1 = np.linalg.norm(v1) \n",
    "        norm_v2 = np.linalg.norm(v2)\n",
    "        # Compute the cosine similarity and append to the list\n",
    "        similarity = dot_product / (norm_v1 * norm_v2)\n",
    "        similarities.append(similarity)\n",
    "    return [float(x) for x in similarities]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b38f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the cosine similarity function to compare the word 'car' with other words\n",
    "word = 'car'\n",
    "print(f\"{word}:\")\n",
    "for i, w in enumerate(words):\n",
    "    # Get the vectorized word for the word defined above\n",
    "    vectorized_word = vectorized_words[words.index(word)]\n",
    "    print(f\"\\t{w}:\\t\\tCosine Similarity: {cosine_similarity(vectorized_word, vectorized_words[i])[0]:.4f}\")\n",
    "print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity between the two vectors\n",
    "def words_cosine_similarity(v1, v2):\n",
    "    # ensure that v1 and v2 are numpy arrays\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    # compute the dot product of v1 and v2\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    # compute the norms of the vectors, np.linalg.norm(vector) computes the length of the vector\n",
    "    norm_v1 = np.linalg.norm(v1) \n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    # compute the cosine similarity\n",
    "    similarity = dot_product / (norm_v1 * norm_v2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0813e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_cow = model.encode(\"cow\")\n",
    "v2_apple = model.encode(\"apple\")\n",
    "v3_alien = model.encode(\"alien\")\n",
    "v4_dog = model.encode(\"dog\")\n",
    "\n",
    "\n",
    "print(\"similarity between apple and alien:\", words_cosine_similarity(v2_apple, v3_alien)) \n",
    "print(\"similarity between cow and alien:\", words_cosine_similarity(v1_cow, v3_alien))\n",
    "print(\"similarity between cow and dog:\", words_cosine_similarity(v1_cow, v4_dog)) \n",
    "print(\"similarity between apple and dog:\", words_cosine_similarity(v2_apple, v4_dog))         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b6533",
   "metadata": {},
   "source": [
    "### Limitation of Local Models: Input size\n",
    "\n",
    "There is a limit to how much text these models can process at once, leading to truncation of text that exceeds this limit. When truncation occurs, all information beyond a certain point in the text is lost, potentially impacting the effectiveness and accuracy of the embedding.\n",
    "\n",
    "To demonstrate this, I will use the `BAAI/bge-base-en-v1.5` model to generate document embeddings for a long text. The model has a maximum input size of 512 tokens, so any text longer than that will be truncated.\n",
    "\n",
    "Note that tokens are not the same as words. A token can be a word, part of a word, or even punctuation. For example, the sentence \"I love pizza!\" might be tokenized into three tokens: \"I\", \"love\", and \"pizza!\". The exact number of tokens in a piece of text can vary depending on the tokenizer used by the model.\n",
    "\n",
    "When we use len(str) to check the length of a long text, it returns the number of `characters` in the text, not the number of `tokens` or `words`. This can lead to confusion, as the number of tokens may be significantly less than the number of characters, especially for longer texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e16ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12181\n"
     ]
    }
   ],
   "source": [
    "long_text = open(\"./resource/long_text.txt\").read()\n",
    "print(len(long_text)) #print the length of the long text, which is the number of characters in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0857091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "[ 0.0412914   0.03247961  0.01876977 -0.04653402  0.04523577]\n",
      "(768,)\n",
      "[ 0.0412914   0.03247961  0.01876977 -0.04653402  0.04523577]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text_embedding = model.encode(long_text, normalize_embeddings=True)\n",
    "print(long_text_embedding.shape)  # Output: (768,), which means the long text is represented by a 768-dimensional vector\n",
    "print(long_text_embedding[:5])  # first 5 elements of the embedding\n",
    "\n",
    "long_text_embedding_truncated = model.encode(long_text[:3000], normalize_embeddings=True)\n",
    "print(long_text_embedding_truncated.shape)  # Output: (768,), which means the truncated long text is also represented by a 768-dimensional vector\n",
    "print(long_text_embedding_truncated[:5])  # first 5 elements of the truncated embedding\n",
    "\n",
    "np.array_equal(long_text_embedding, long_text_embedding_truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767472f5",
   "metadata": {},
   "source": [
    "When the text is longer than the model's maximum input size, the model will truncate the text to fit within the limit. This means that any information beyond a certain point in the text will be lost, potentially impacting the effectiveness and accuracy of the embedding. \n",
    "\n",
    "However, if the truncated text is still within the model's maximum input size, the model will generate an embedding for the truncated text without any issues. In this case, the embedding will be different from the embedding of the original long text, as the truncated text may not contain all the information present in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79f1be4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maggiezhao/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "[ 0.03915093  0.00793503  0.03163996 -0.05213862  0.05705126]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text_small_truncated = model.encode(long_text[:1000], normalize_embeddings=True)\n",
    "print(long_text_small_truncated.shape)  # Output: (768,), which means the small truncated long text is also represented by a 768-dimensional vector\n",
    "print(long_text_small_truncated[:5])  # first 5 elements of the small truncated embedding\n",
    "np.array_equal(long_text_embedding, long_text_small_truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef4e15",
   "metadata": {},
   "source": [
    "## Models for Embeddings: commercial options\n",
    "\n",
    "Besides the local models, there are also commercial options available for generating document embeddings. These models are typically hosted by companies and can be accessed via APIs. They often provide higher accuracy and better performance compared to local models, but they may come with usage costs. In addition, since they are hosted by third-party companies, there may be concerns about data privacy, security, and accessibility.\n",
    "\n",
    "I use OpenAI's API for generating embeddings in my projects. It provides a simple interface and high-quality embeddings, making it a great choice for many applications.\n",
    "\n",
    "It is important to note that the choice of model for generating document embeddings can significantly impact the performance of your RAG system, and once you choose a model, you should stick with it throughout your project to ensure consistency in the embeddings. Switching models mid-project can lead to inconsistencies in the embeddings and may require re-embedding all your documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7dac38",
   "metadata": {},
   "source": [
    "## Data Storage: FAISS vs Chroma \n",
    "\n",
    "If we have a lot of documents, it would be inefficient to store all the document embeddings in memory. Instead, we can use a `vector database` to store and retrieve the embeddings efficiently.\n",
    "\n",
    "The choice of vector database confused me a lot when I first started learning about RAG systems. These terms were, and still are so abstract to me that I decided to just listen to the recommendations from the course and from ChatGPT. As far as I am concerned, I should be fine with either FAISS or Chroma, as they are both popular choices for storing and retrieving vectors.\n",
    "\n",
    "My application is very small and personal, so I figure I don't need to worry about scalability or performance issues as I wouldn't be able to tell the difference anyway. I will use Chroma, together with LangChain and OpenAI's commercial API, to build my simple RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54422904",
   "metadata": {},
   "source": [
    "## Metadata \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1f63b",
   "metadata": {},
   "source": [
    "`Metadata` is additional information about the data that can be used to enhance the search and retrieval process. It is easier to understand with an example. In the context of a RAG system, metadata can include information such as:\n",
    "\n",
    "- `Document title:` The title of the document, which can help users quickly identify the content of the document.\n",
    "\n",
    "- `Document author:` The author of the document, which can help users identify the credibility and reliability of the information.\n",
    "\n",
    "- `Document date:` The date when the document was created or last modified, which can help users determine the relevance and timeliness of the information.\n",
    "\n",
    "- `Document tags:` Tags or keywords associated with the document, which can help users quickly find related documents or topics.\n",
    "\n",
    "- `Document source:` The source of the document, which can help users identify the origin and context of the information.\n",
    "\n",
    "- `Document summary:` A brief summary of the document's content, which can help users quickly understand the main points of the document without reading the entire text.\n",
    "\n",
    "- `Document language:` The language in which the document is written, which can help users filter documents based on their language preferences.\n",
    "\n",
    "- `Document category:` The category or type of the document, which can help users organize and classify documents based on their content or purpose.\n",
    "\n",
    "- `Document length:` The length of the document, which can help users determine the amount of information contained in the document and whether it is suitable for their needs.\n",
    "\n",
    "- `Document format:` The format of the document (e.g., PDF, Word, HTML), which can help users understand how to access and view the document.\n",
    "\n",
    "- `Document keywords:` Specific keywords or phrases associated with the document, which can help users find documents related to specific topics or queries.\n",
    "\n",
    "- ....(and many more)\n",
    "\n",
    "This list can go on and on, since it is highly dependent on the specific use case and the type of data being stored. The key point is that metadata provides additional context and information about the data, which can enhance the search and retrieval process in a RAG system.\n",
    "\n",
    "What does metadata look like in practice? It is typically stored as a dictionary or JSON object, where each key represents a specific piece of metadata and the corresponding value contains the relevant information. For example, here is a sample metadata dictionary for a document:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"title\": \"The Great Gatsby\",\n",
    "    \"author\": \"F. Scott Fitzgerald\",\n",
    "    \"date\": \"1925-04-10\",\n",
    "    \"tags\": [\"classic\", \"novel\", \"American literature\"],\n",
    "    \"source\": \"https://example.com/great-gatsby\",\n",
    "    \"summary\": \"A novel set in the 1920s that explores themes of wealth, love, and the American Dream.\",\n",
    "    \"language\": \"English\",\n",
    "    \"category\": \"Fiction\",\n",
    "    \"length\": 180,\n",
    "    \"format\": \"PDF\",\n",
    "    \"keywords\": [\"Gatsby\", \"Daisy\", \"Jay\"]\n",
    "}\n",
    "```\n",
    "\n",
    "Metadata is usually created during the data ingestion process, where each document is processed and its metadata is extracted and stored alongside the document's content. This metadata can then be used to filter, sort, and enhance search results in the RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb757c23",
   "metadata": {},
   "source": [
    "For PDFs, you can usually extract some metadata such as title, author, and subject. This can be done using libraries like PyPDF2 or pdfminer in Python. The extracted metadata can then be used to create a more informative vector representation of the document. \n",
    "\n",
    "For other applications, you can try to generate metadata based on the content and context of the document using LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1499497",
   "metadata": {},
   "source": [
    "### Metadata filter example from coursera\n",
    "The following functions are from the course I am taking, and they won't run without editing, I put the code here as a reference for the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e713d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_params_dict(\n",
    "    prompt: str, \n",
    "    temperature: float = None, \n",
    "    role = 'user',\n",
    "    top_p: float = None,\n",
    "    max_tokens: int = 500,\n",
    "    model: str = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Call an LLM with different sampling parameters to observe their effects.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The text prompt to send to the model\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        top_p: Controls diversity via nucleus sampling\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        model: The model to use\n",
    "        \n",
    "    Returns:\n",
    "        The LLM response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the dictionary with the necessary parameters\n",
    "    kwargs = {\"prompt\": prompt, 'role':role, \"temperature\": temperature, \"top_p\": top_p, \"max_tokens\": max_tokens, 'model': model} \n",
    "\n",
    "\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_single_input(prompt: str, role: str = 'user', top_p: float = None, temperature: float = None,\n",
    "                               max_tokens: int = 500, model: str = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\",\n",
    "                               together_api_key=None, **kwargs):\n",
    "    if top_p is None:\n",
    "        top_p = 'none'\n",
    "    if temperature is None:\n",
    "        temperature = 'none'\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{'role': role, 'content': prompt}],\n",
    "        \"top_p\": top_p,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        **kwargs\n",
    "    }\n",
    "    if (not together_api_key) and ('TOGETHER_API_KEY' not in os.environ):\n",
    "        url = os.path.join('https://proxy.dlai.link/coursera_proxy/together', 'v1/chat/completions')\n",
    "        response = requests.post(url, json=payload, verify=False)\n",
    "        if not response.ok:\n",
    "            raise Exception(f\"Error while calling LLM: f{response.text}\")\n",
    "        try:\n",
    "            json_dict = json.loads(response.text)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to get correct output from LLM call.\\nException: {e}\\nResponse: {response.text}\")\n",
    "    else:\n",
    "        if together_api_key is None:\n",
    "            together_api_key = os.environ['TOGETHER_API_KEY']\n",
    "        client = Together(api_key=together_api_key)\n",
    "        json_dict = client.chat.completions.create(**payload).model_dump()\n",
    "        json_dict['choices'][-1]['message']['role'] = json_dict['choices'][-1]['message']['role'].name.lower()\n",
    "    try:\n",
    "        output_dict = {'role': json_dict['choices'][-1]['message']['role'],\n",
    "                       'content': json_dict['choices'][-1]['message']['content']}\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to get correct output dict. Please try again. Error: {e}\")\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab5e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metadata_from_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates metadata in JSON format based on a given query to filter clothing items.\n",
    "    Returns a JSON string with keys: gender, masterCategory, articleType, baseColour, price, usage, season.\n",
    "    \"\"\"\n",
    "\n",
    "# Prompt with schema, allowed values, and strict output constraints\n",
    "    prompt = f\"\"\"\n",
    "You translate a shopping query into a strict JSON filter for a vector DB.\n",
    "\n",
    "Output rules:\n",
    "- Return ONLY valid JSON. No prose. No code fences. One JSON object.\n",
    "- Keys (always present): \"gender\", \"masterCategory\", \"articleType\", \"baseColour\", \"price\", \"usage\", \"season\".\n",
    "- Values for all keys except \"price\" must be lists of strings.\n",
    "- \"price\" must be an object with numeric \"min\" and \"max\". If not specified, use {{\"min\": 0, \"max\": \"inf\"}}.\n",
    "- If the query does not constrain a category, set it to [\"Any\"].\n",
    "- Normalize to the allowed vocab below. If the query uses synonyms, map to the closest allowed value.\n",
    "\n",
    "Allowed values:\n",
    "- gender: [\"Men\",\"Women\",\"Boys\",\"Girls\",\"Unisex\"]\n",
    "- masterCategory: [\"Apparel\",\"Footwear\",\"Accessories\"]\n",
    "- articleType: [\"Tshirts\",\"Shirts\",\"Tops\",\"Dresses\",\"Trousers\",\"Jeans\"]\n",
    "- baseColour: [\"Black\",\"White\",\"Blue\",\"Red\",\"Green\",\"Yellow\",\"Pink\",\"Purple\",\"Orange\",\"Brown\",\"Grey\",\"Beige\",\"Navy\",\"Maroon\",\"Multi\"]\n",
    "- usage: [\"Casual\",\"Formal\",\"Sports\",\"Party\",\"Travel\",\"Work\",\"Ethnic\"]\n",
    "- season: [\"Summer\",\"Winter\",\"Spring\",\"Autumn\",\"All\"]\n",
    "\n",
    "Price parsing guidance:\n",
    "- \"under/less than X\" -> {{\"min\": 0, \"max\": X}}\n",
    "- \"over/above/greater than X\" -> {{\"min\": X, \"max\": \"inf\"}}\n",
    "- \"between X and Y\", \"X–Y\" -> {{\"min\": X, \"max\": Y}}\n",
    "- Currency symbols may appear; ignore currency and parse numbers.\n",
    "- If no price mentioned -> {{\"min\": 0, \"max\": \"inf\"}}.\n",
    "\n",
    "Mapping hints:\n",
    "- \"women's\"/\"for women\" -> \"Women\"; \"men's\" -> \"Men\"; \"unisex\" -> \"Unisex\".\n",
    "- Occasion words: \"wedding\" -> [\"Party\",\"Formal\"]; \"office/work\" -> [\"Work\"]; \"running/gym\" -> [\"Sports\"];\n",
    "  \"beach/vacation\" -> [\"Travel\"]; \"daily/everyday\" -> [\"Casual\"].\n",
    "- Weather: \"summer/hot\" -> [\"Summer\"]; \"winter/cold\" -> [\"Winter\"]; \"spring\" -> [\"Spring\"]; \"fall/autumn\" -> [\"Autumn\"].\n",
    "- Colors: map to closest in baseColour; if mixed/print -> [\"Multi\"].\n",
    "- If item family implies masterCategory (e.g., Sneakers -> Footwear), set masterCategory accordingly.\n",
    "    \n",
    "UserQuery: \"{query}\"\n",
    "\n",
    "Return ONLY JSON, no text, no ```json ``` quotation marks;\n",
    "\n",
    "key:value example--> \n",
    "\"gender\": [\"Men\"],\n",
    "\"masterCategory\": [\"Apparel\"]\n",
    "\n",
    "Example of expected JSON:\n",
    "\n",
    "{{\n",
    "  \"gender\": [\"Women\"],\n",
    "  \"masterCategory\": [\"Apparel\"],\n",
    "  \"articleType\": [\"Dresses\"],\n",
    "  \"baseColour\": [\"Blue\"],\n",
    "  \"price\": {{\"min\": 0, \"max\": \"inf\"}},\n",
    "  \"usage\": [\"Formal\"],\n",
    "  \"season\": [\"All seasons\"]\n",
    "}}\n",
    "Return ONLY JSON, no text, no ```json ``` quotation marks;\n",
    "\"\"\".strip()\n",
    "\n",
    "    # Generate with low randomness and ample tokens\n",
    "    gen_kwargs = generate_params_dict(prompt=prompt, temperature=0, max_tokens=1500)\n",
    "    response = generate_with_single_input(**gen_kwargs)\n",
    "\n",
    "    # Extract raw JSON string\n",
    "    content = response[\"content\"]\n",
    "\n",
    "\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eecbecc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_params_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(generate_metadata_from_query(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate a look for a man that suits a sunny day in the park. I don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt want to spend more than 300 dollars on each piece.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[1], line 65\u001b[0m, in \u001b[0;36mgenerate_metadata_from_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      8\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mYou translate a shopping query into a strict JSON filter for a vector DB.\u001b[39m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124mReturn ONLY JSON, no text, no ```json ``` quotation marks;\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Generate with low randomness and ample tokens\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     gen_kwargs \u001b[38;5;241m=\u001b[39m generate_params_dict(prompt\u001b[38;5;241m=\u001b[39mprompt, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m)\n\u001b[1;32m     66\u001b[0m     response \u001b[38;5;241m=\u001b[39m generate_with_single_input(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Extract raw JSON string\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_params_dict' is not defined"
     ]
    }
   ],
   "source": [
    "print(generate_metadata_from_query(\"Create a look for a man that suits a sunny day in the park. I don't want to spend more than 300 dollars on each piece.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115cea44",
   "metadata": {},
   "source": [
    "We sometimes need to parse the JSON output from the LLM to extract useful information. This can be done using the `parse_json_output` function defined earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_output(llm_output: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parses a string output from an LLM into a JSON object.\n",
    "\n",
    "    This function attempts to clean and parse a JSON-formatted string produced by an LLM.\n",
    "    The input string might contain minor formatting issues, such as unnecessary newlines or single quotes\n",
    "    instead of double quotes. The function attempts to correct such issues before parsing.\n",
    "\n",
    "    Parameters:\n",
    "    - llm_output (str): The string output from the LLM that is expected to be in JSON format.\n",
    "\n",
    "    Returns:\n",
    "    - dict or None: A dictionary if parsing is successful, or None if the input string cannot be parsed into valid JSON.\n",
    "\n",
    "    Exception Handling:\n",
    "    - In case of a JSONDecodeError during parsing, an error message is printed, and the function returns None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Since the input might be improperly formatted, ensure any single quotes are removed\n",
    "        llm_output = llm_output.replace(\"\\n\", '').replace(\"'\",'').replace(\"}}\", \"}\").replace(\"{{\", \"{\")  # Remove any erroneous structures\n",
    "        \n",
    "        # Attempt to parse JSON directly provided it is a properly-structured JSON string\n",
    "        parsed_json = json.loads(llm_output)\n",
    "        return parsed_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = generate_metadata_from_query(\"Give me three blue dresses suitable for a wedding party, less than 200 dollars and at least 50 dollars\")\n",
    "json_output = parse_json_output(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39ce40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dc8ebe1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter_by_metadata(json_output: dict | None = None):\n",
    "    \"\"\"\n",
    "    Generate a list of Weaviate filters based on a provided metadata dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - json_output (dict) or None: Dictionary containing metadata keys and their values.\n",
    "\n",
    "    Returns:\n",
    "    - list[Filter] or None: A list of Weaviate filters, or None if input is None.\n",
    "    \"\"\"\n",
    "    # If the input dictionary is None, return None immediately\n",
    "    if json_output is None:\n",
    "        return None\n",
    "\n",
    "    # Define a tuple of valid keys that are allowed for filtering\n",
    "    valid_keys = (\n",
    "        'gender',\n",
    "        'masterCategory',\n",
    "        'articleType',\n",
    "        'baseColour',\n",
    "        'price',\n",
    "        'usage',\n",
    "        'season',\n",
    "    )\n",
    "\n",
    "    # Initialize an empty list to store the filters\n",
    "    filters = []\n",
    "\n",
    "    # Iterate over each key-value pair in the input dictionary\n",
    "    for key, value in json_output.items():\n",
    "        # Skip the key if it is not in the list of valid keys\n",
    "        if key not in valid_keys:\n",
    "            continue\n",
    "\n",
    "        # Special handling for the 'price' key\n",
    "        if key == 'price':\n",
    "            # Ensure the value associated with 'price' is a dictionary\n",
    "            if not isinstance(value, dict):\n",
    "                continue\n",
    "\n",
    "            # Extract the minimum and maximum prices from the dictionary\n",
    "            min_price = value.get('min')\n",
    "            max_price = value.get('max')\n",
    "\n",
    "            # Skip if either min_price or max_price is not provided\n",
    "            if min_price is None or max_price is None:\n",
    "                continue\n",
    "\n",
    "            # Skip if min_price is non-positive or max_price is infinity\n",
    "            if min_price <= 0 or max_price == 'inf':\n",
    "                continue\n",
    "\n",
    "            # Add filters for price greater than min_price and less than max_price\n",
    "            filters.append(Filter.by_property(key).greater_than(min_price))\n",
    "            filters.append(Filter.by_property(key).less_than(max_price))\n",
    "        else:\n",
    "            # For other valid keys, add a filter that checks for any of the provided values\n",
    "            filters.append(Filter.by_property(key).contains_any(value))\n",
    "\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filters_from_query(query: str) -> list:\n",
    "    json_string = generate_metadata_from_query(query)\n",
    "    json_output = parse_json_output(json_string)\n",
    "    filters = get_filter_by_metadata(json_output)\n",
    "    return filters\n",
    "filters = generate_filters_from_query(\"Give me three T-shirts to use in sunny days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_products_from_query(query: str):\n",
    "    \"\"\"\n",
    "    Retrieve products that are most relevant to a given query by applying filters.\n",
    "\n",
    "    This function generates filters based on the provided query and uses them to find \n",
    "    products that closely match the query criteria. If no filters are applicable or if \n",
    "    the initial search returns a small number of products, the function dynamically reduces \n",
    "    the filtering constraints based on a predefined order of filter importance.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The query string used to search for relevant products.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of product objects that are most relevant to the query. If filters are not effective,\n",
    "          it adjusts them to ensure a minimum return of products.\n",
    "    \"\"\"\n",
    "    filters = generate_filters_from_query(query)  # Generate filters based on query\n",
    "\n",
    "    # Check if there are no applicable filters\n",
    "    if filters is None or len(filters) == 0:\n",
    "        # Query the collection without filters, using the query text for relevance\n",
    "        res = products_collection.query.near_text(query, limit=20).objects\n",
    "        return res\n",
    "\n",
    "    # Query with filters and limit to top 20 relevant objects\n",
    "    res = products_collection.query.near_text(query, filters=Filter.all_of(filters), limit=20).objects\n",
    "\n",
    "    # If the result set is fewer than 10 products, try reducing filters to broaden the search\n",
    "    importance_order = ['baseColour', 'masterCategory', 'usage', 'masterCategory', 'season', 'gender']\n",
    "\n",
    "    if len(res) < 10:\n",
    "        # Iterate through the importance order of filters\n",
    "        for i in range(len(importance_order)):\n",
    "            # Create a list of filters that excludes less important ones\n",
    "            filtered_filters = [x for x in filters if x.target not in importance_order[i+1:]]\n",
    "            \n",
    "            # Re-query with the reduced set of filters\n",
    "            res = products_collection.query.near_text(query, filters=Filter.all_of(filtered_filters), limit=20).objects\n",
    "            \n",
    "            # If sufficient products have been found, return early\n",
    "            if len(res) >= 5:\n",
    "                return res\n",
    "\n",
    "    return res  # Return the final set of relevant products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe73d1",
   "metadata": {},
   "source": [
    "## Quick refresher note on slicing numpy arrays\n",
    "\n",
    "The basic syntax for slicing numpy arrays is `arr[start:stop:step(first dimension), start:stop:step(second dimension)...]`\n",
    "\n",
    "- `start` is the index to start slicing from (inclusive)\n",
    "  \n",
    "- `stop` is the index to stop slicing at (exclusive)\n",
    " \n",
    "- `step` is the step size (default is 1)\n",
    "  \n",
    "- if you omit start, it defaults to 0, and the slice looks like arr[:stop], the step is optional, and if you omit it, it defaults to 1, so arr[start:stop] is equivalent to arr[start:stop:1]\n",
    "  \n",
    "- if you use [:], it means all elements along that dimension, for example, arr[:, :] means all rows and all columns and  arr[:, 0:2] means all rows and the first two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e487381d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array:\n",
      " [[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]\n",
      " [11 12 13 14 15]\n",
      " [16 17 18 19 20]]\n",
      "First row: [1 2 3 4 5]\n",
      "==============================\n",
      "First two rows:\n",
      " [[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]]\n",
      "==============================\n",
      "First two columns:\n",
      " [[ 1  2]\n",
      " [ 6  7]\n",
      " [11 12]\n",
      " [16 17]]\n",
      "==============================\n",
      "First two rows and columns:\n",
      " [[1 2]\n",
      " [6 7]]\n",
      "==============================\n",
      "Last element: 20\n"
     ]
    }
   ],
   "source": [
    "# quick note on slicing numpy arrays\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create a 4*5 numpy array\n",
    "arr = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20]])\n",
    "\n",
    "# Slice the first row; \n",
    "first_row = arr[0]\n",
    "\n",
    "# Slice the first two rows \n",
    "first_two_rows = arr[0:2]\n",
    "\n",
    "# Slice the first two columns, [:, 0:2] means all rows and columns 0 and 1\n",
    "first_two_columns = arr[:, 0:2] # Slice all elements in the first two columns \n",
    "\n",
    "# Slice the first two rows and first two columns\n",
    "first_two_rows_and_columns = arr[0:2, 0:2]\n",
    "last_element = arr[-1, -1]  # Access the last element in the array\n",
    "\n",
    "# formatting the output\n",
    "print(\"Array:\\n\", arr)\n",
    "print(\"First row:\", first_row)\n",
    "print(\"==\"*15)\n",
    "print(\"First two rows:\\n\", first_two_rows)  \n",
    "print(\"==\"*15)\n",
    "print(\"First two columns:\\n\", first_two_columns)\n",
    "print(\"==\"*15)\n",
    "print(\"First two rows and columns:\\n\", first_two_rows_and_columns)\n",
    "print(\"==\"*15)\n",
    "print(\"Last element:\", last_element)  # Output: 20\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
