{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4977d4c",
   "metadata": {},
   "source": [
    "# Vector Database and the Use of Metadata\n",
    "\n",
    "## Framework of RAG\n",
    "\n",
    "1. **Create a vector store:** This involves embedding documents into a vector space using a model like Sentence Transformers or OpenAI's text-embedding-ada-002.\n",
    "\n",
    "2. **Add documents to the vector store:** This step involves storing the embedded documents in a vector database such as Pinecone, Weaviate, or a local vector store like FAISS.\n",
    "\n",
    "3. **Query the vector store by calculating similarity between the query and the documents:** This is done by embedding the query and comparing it to the stored document vectors using cosine similarity or other distance metrics.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./resource/vec.jpg\" alt=\"Vector Search\" width=\"800\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a8ea1",
   "metadata": {},
   "source": [
    "## Vectors: word embeddings vs. document embeddings\n",
    "\n",
    "Vectors are numerical representations of words or documents in a high-dimensional space. They capture semantic meaning, allowing for similarity comparisons. But there are two types of vectors that can easily confuse beginners:\n",
    "\n",
    "1. `Word embeddings:` These are vectors representing individual words, capturing their meanings and relationships. They are typically generated using models like Word2Vec or GloVe.\n",
    "\n",
    "2. `Document embeddings:` These vectors represent entire documents, capturing the overall meaning and context. They are often generated by aggregating word embeddings or using models like Sentence Transformers or OpenAI's text-embedding-ada-002.\n",
    "\n",
    "For RAG systems, we primarily use document embeddings, as they allow us to compare the meaning of entire documents rather than just individual words. Note that the `\"document\"` in this context can be a sentence, paragraph, or even an entire article, depending on the granularity of the information we want to retrieve. It is NOT limited to what we usually think of as a \"document\" in the traditional sense. This confused the hell out of me when I first learned about RAG systems, so I want to clarify it here.\n",
    "\n",
    "There are many models available for generating document embeddings, each with its own strengths and weaknesses. \n",
    "\n",
    "Table 5.1 Popular models for document embeddings\n",
    "| Model Name | Description | Use Case |Local Option | Commercial Option |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| OpenAI's text-embedding-ada-002 | A powerful model for generating document embeddings. | General-purpose document embedding. | No | Yes |\n",
    "| BAAI/bge-base-en-v1.5 | A model designed for generating document embeddings. | General-purpose document embedding. | Yes | No |\n",
    "| intfloat/e5-base | A model designed for generating document embeddings. | General-purpose document embedding. | Yes | No |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea142c9",
   "metadata": {},
   "source": [
    "I will use sentence-transformers' `BAAI/bge-base-en-v1.5` model to generate document embeddings in this notebook. This model is designed for generating document embeddings and is available for local use, making it a good choice for many applications. It will convert text into a 768-dimensional vector in numpy ndarray format, which is a common size for document embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110dfff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y keras keras-nightly keras-preprocessing keras-vis\n",
    "!pip uninstall -y tf-keras-nightly tf-keras\n",
    "!pip install tf-keras --upgrade\n",
    "!pip install --upgrade transformers sentence-transformers\n",
    "# install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('BAAI/bge-base-en-v1.5') \n",
    "# assign the model to a variable, if the model is not downloaded, it will be downloaded automatically\n",
    "# so the model is running locally, it's not a big model and can be run on a laptop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"query: What are the symptoms of COVID-19?\"]\n",
    "embedding = model.encode(query, normalize_embeddings=True)\n",
    "print(embedding.shape)  # Output: (1, 768), which means the query is represented by a 768-dimensional vector\n",
    "print(embedding[:, :5])  # first 5 elements of the embedding\n",
    "# model.encode(str) returns a numpy array which represents the embedding of the input text\n",
    "# the bge-base-en-v1.5 model produces 768-dimensional embeddings ((768,) means a 1D array with 768 elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78699973",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.encode(\"RAG is awesome\") \n",
    "#model.encode(str) returns a numpy array which represents the embedding of the input text\n",
    "print(res.shape) # Output: (768,) \n",
    "# the bge-base-en-v1.5 model produces 768-dimensional embeddings ((768,) means a 1D array with 768 elements)\n",
    "print(res[:5]) # first 5 elements of the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d247ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of encoding multiple words\n",
    "# We can pass a list of words to the model to get their embeddings\n",
    "\n",
    "words = ['apple', 'car', 'fruit', 'automobile', 'love', 'sentiment','book']\n",
    "vectorized_words = model.encode(words) \n",
    "# returns a 2D numpy array where each row is the embedding of a word\n",
    "\n",
    "print(vectorized_words.shape) \n",
    "# Output: (7, 768), which means there are 7 words, each represented by a 768-dimensional vector\n",
    "\n",
    "print(vectorized_words[:5,:5]) # first 5 word embeddings' first 5 elements\n",
    "# there's a note on numpy slicing at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dbd1c1",
   "metadata": {},
   "source": [
    "[→Jump to numpy slicing reference←](#quick-refresher-note-on-slicing-numpy-arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity Function\n",
    "def cosine_similarity(v1, array_of_vectors):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between a vector and an array of vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    v1 (array-like): The first vector.\n",
    "    array_of_vectors (array-like): An array of vectors or a single vector.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of cosine similarities between v1 and each vector in array_of_vectors.\n",
    "    \"\"\"\n",
    "    # Ensure that v1 is a numpy array\n",
    "    v1 = np.array(v1)\n",
    "    # Initialize a list to store similarities\n",
    "    similarities = []\n",
    "    \n",
    "    # Check if array_of_vectors is a single vector\n",
    "    if len(np.shape(array_of_vectors)) == 1:\n",
    "        array_of_vectors = [array_of_vectors]\n",
    "    \n",
    "    # Iterate over each vector in the array\n",
    "    for v2 in array_of_vectors:\n",
    "        # Convert the current vector to a numpy array\n",
    "        v2 = np.array(v2)\n",
    "        # Compute the dot product of v1 and v2\n",
    "        dot_product = np.dot(v1, v2)\n",
    "        # Compute the norms of the vectors, np.linalg.norm(vector) computes the length of the vector\n",
    "        norm_v1 = np.linalg.norm(v1) \n",
    "        norm_v2 = np.linalg.norm(v2)\n",
    "        # Compute the cosine similarity and append to the list\n",
    "        similarity = dot_product / (norm_v1 * norm_v2)\n",
    "        similarities.append(similarity)\n",
    "    return [float(x) for x in similarities]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b38f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the cosine similarity function to compare the word 'car' with other words\n",
    "word = 'car'\n",
    "print(f\"{word}:\")\n",
    "for i, w in enumerate(words):\n",
    "    # Get the vectorized word for the word defined above\n",
    "    vectorized_word = vectorized_words[words.index(word)]\n",
    "    print(f\"\\t{w}:\\t\\tCosine Similarity: {cosine_similarity(vectorized_word, vectorized_words[i])[0]:.4f}\")\n",
    "print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity between the two vectors\n",
    "def words_cosine_similarity(v1, v2):\n",
    "    # ensure that v1 and v2 are numpy arrays\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    # compute the dot product of v1 and v2\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    # compute the norms of the vectors, np.linalg.norm(vector) computes the length of the vector\n",
    "    norm_v1 = np.linalg.norm(v1) \n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    # compute the cosine similarity\n",
    "    similarity = dot_product / (norm_v1 * norm_v2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0813e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_cow = model.encode(\"cow\")\n",
    "v2_apple = model.encode(\"apple\")\n",
    "v3_alien = model.encode(\"alien\")\n",
    "v4_dog = model.encode(\"dog\")\n",
    "\n",
    "\n",
    "print(\"similarity between apple and alien:\", words_cosine_similarity(v2_apple, v3_alien)) \n",
    "print(\"similarity between cow and alien:\", words_cosine_similarity(v1_cow, v3_alien))\n",
    "print(\"similarity between cow and dog:\", words_cosine_similarity(v1_cow, v4_dog)) \n",
    "print(\"similarity between apple and dog:\", words_cosine_similarity(v2_apple, v4_dog))         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b6533",
   "metadata": {},
   "source": [
    "### Limitation of Local Models: Input size\n",
    "\n",
    "There is a limit to how much text these models can process at once, leading to truncation of text that exceeds this limit. When truncation occurs, all information beyond a certain point in the text is lost, potentially impacting the effectiveness and accuracy of the embedding.\n",
    "\n",
    "To demonstrate this, I will use the `BAAI/bge-base-en-v1.5` model to generate document embeddings for a long text. The model has a maximum input size of 512 tokens, so any text longer than that will be truncated.\n",
    "\n",
    "Note that tokens are not the same as words. A token can be a word, part of a word, or even punctuation. For example, the sentence \"I love pizza!\" might be tokenized into three tokens: \"I\", \"love\", and \"pizza!\". The exact number of tokens in a piece of text can vary depending on the tokenizer used by the model.\n",
    "\n",
    "When we use len(str) to check the length of a long text, it returns the number of `characters` in the text, not the number of `tokens` or `words`. This can lead to confusion, as the number of tokens may be significantly less than the number of characters, especially for longer texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e16ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12181\n"
     ]
    }
   ],
   "source": [
    "long_text = open(\"./resource/long_text.txt\").read()\n",
    "print(len(long_text)) #print the length of the long text, which is the number of characters in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0857091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "[ 0.0412914   0.03247961  0.01876977 -0.04653402  0.04523577]\n",
      "(768,)\n",
      "[ 0.0412914   0.03247961  0.01876977 -0.04653402  0.04523577]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text_embedding = model.encode(long_text, normalize_embeddings=True)\n",
    "print(long_text_embedding.shape)  # Output: (768,), which means the long text is represented by a 768-dimensional vector\n",
    "print(long_text_embedding[:5])  # first 5 elements of the embedding\n",
    "\n",
    "long_text_embedding_truncated = model.encode(long_text[:3000], normalize_embeddings=True)\n",
    "print(long_text_embedding_truncated.shape)  # Output: (768,), which means the truncated long text is also represented by a 768-dimensional vector\n",
    "print(long_text_embedding_truncated[:5])  # first 5 elements of the truncated embedding\n",
    "\n",
    "np.array_equal(long_text_embedding, long_text_embedding_truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767472f5",
   "metadata": {},
   "source": [
    "When the text is longer than the model's maximum input size, the model will truncate the text to fit within the limit. This means that any information beyond a certain point in the text will be lost, potentially impacting the effectiveness and accuracy of the embedding. \n",
    "\n",
    "However, if the truncated text is still within the model's maximum input size, the model will generate an embedding for the truncated text without any issues. In this case, the embedding will be different from the embedding of the original long text, as the truncated text may not contain all the information present in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79f1be4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maggiezhao/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "[ 0.03915093  0.00793503  0.03163996 -0.05213862  0.05705126]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text_small_truncated = model.encode(long_text[:1000], normalize_embeddings=True)\n",
    "print(long_text_small_truncated.shape)  # Output: (768,), which means the small truncated long text is also represented by a 768-dimensional vector\n",
    "print(long_text_small_truncated[:5])  # first 5 elements of the small truncated embedding\n",
    "np.array_equal(long_text_embedding, long_text_small_truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef4e15",
   "metadata": {},
   "source": [
    "## Models for Embeddings: commercial options\n",
    "\n",
    "Besides the local models, there are also commercial options available for generating document embeddings. These models are typically hosted by companies and can be accessed via APIs. They often provide higher accuracy and better performance compared to local models, but they may come with usage costs. In addition, since they are hosted by third-party companies, there may be concerns about data privacy, security, and accessibility.\n",
    "\n",
    "I use OpenAI's API for generating embeddings in my projects. It provides a simple interface and high-quality embeddings, making it a great choice for many applications.\n",
    "\n",
    "It is important to note that the choice of model for generating document embeddings can significantly impact the performance of your RAG system, and once you choose a model, you should stick with it throughout your project to ensure consistency in the embeddings. Switching models mid-project can lead to inconsistencies in the embeddings and may require re-embedding all your documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7dac38",
   "metadata": {},
   "source": [
    "## Data Storage: FAISS vs Chroma \n",
    "\n",
    "If we have a lot of documents, it would be inefficient to store all the document embeddings in memory. Instead, we can use a `vector database` to store and retrieve the embeddings efficiently.\n",
    "\n",
    "The choice of vector database confused me a lot when I first started learning about RAG systems. These terms were, and still are so abstract to me that I decided to just listen to the recommendations from the course and from ChatGPT. As far as I am concerned, I should be fine with either FAISS or Chroma, as they are both popular choices for storing and retrieving vectors.\n",
    "\n",
    "My application is very small and personal, so I figure I don't need to worry about scalability or performance issues as I wouldn't be able to tell the difference anyway. I will use Chroma, together with LangChain and OpenAI's commercial API, to build my simple RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55189e50",
   "metadata": {},
   "source": [
    "## Langchain: beginner's guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54422904",
   "metadata": {},
   "source": [
    "## Metadata \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe73d1",
   "metadata": {},
   "source": [
    "## Quick refresher note on slicing numpy arrays\n",
    "\n",
    "The basic syntax for slicing numpy arrays is `arr[start:stop:step(first dimension), start:stop:step(second dimension)...]`\n",
    "\n",
    "- `start` is the index to start slicing from (inclusive)\n",
    "  \n",
    "- `stop` is the index to stop slicing at (exclusive)\n",
    " \n",
    "- `step` is the step size (default is 1)\n",
    "  \n",
    "- if you omit start, it defaults to 0, and the slice looks like arr[:stop], the step is optional, and if you omit it, it defaults to 1, so arr[start:stop] is equivalent to arr[start:stop:1]\n",
    "  \n",
    "- if you use [:], it means all elements along that dimension, for example, arr[:, :] means all rows and all columns and  arr[:, 0:2] means all rows and the first two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e487381d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array:\n",
      " [[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]\n",
      " [11 12 13 14 15]\n",
      " [16 17 18 19 20]]\n",
      "First row: [1 2 3 4 5]\n",
      "==============================\n",
      "First two rows:\n",
      " [[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]]\n",
      "==============================\n",
      "First two columns:\n",
      " [[ 1  2]\n",
      " [ 6  7]\n",
      " [11 12]\n",
      " [16 17]]\n",
      "==============================\n",
      "First two rows and columns:\n",
      " [[1 2]\n",
      " [6 7]]\n",
      "==============================\n",
      "Last element: 20\n"
     ]
    }
   ],
   "source": [
    "# quick note on slicing numpy arrays\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create a 4*5 numpy array\n",
    "arr = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20]])\n",
    "\n",
    "# Slice the first row; \n",
    "first_row = arr[0]\n",
    "\n",
    "# Slice the first two rows \n",
    "first_two_rows = arr[0:2]\n",
    "\n",
    "# Slice the first two columns, [:, 0:2] means all rows and columns 0 and 1\n",
    "first_two_columns = arr[:, 0:2] # Slice all elements in the first two columns \n",
    "\n",
    "# Slice the first two rows and first two columns\n",
    "first_two_rows_and_columns = arr[0:2, 0:2]\n",
    "last_element = arr[-1, -1]  # Access the last element in the array\n",
    "\n",
    "# formatting the output\n",
    "print(\"Array:\\n\", arr)\n",
    "print(\"First row:\", first_row)\n",
    "print(\"==\"*15)\n",
    "print(\"First two rows:\\n\", first_two_rows)  \n",
    "print(\"==\"*15)\n",
    "print(\"First two columns:\\n\", first_two_columns)\n",
    "print(\"==\"*15)\n",
    "print(\"First two rows and columns:\\n\", first_two_rows_and_columns)\n",
    "print(\"==\"*15)\n",
    "print(\"Last element:\", last_element)  # Output: 20\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
