{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6741be64",
   "metadata": {},
   "source": [
    "# Q&A during my study\n",
    "\n",
    "I decided to keep track of all my confusions and questions during my study, many of which are usually not answered in the cookbooks or tutorials I found. I guess the reason is that these questions tend to be too obvious or too trivial to people with technical background or experience. For beginners from another field (me), the entire world of IT poses a flow of questions that will annoy \"experts\".\n",
    "\n",
    "The Q&A is not structured in any specific order, just a collection of my questions and answers. The answers are usually from my own understanding but fortified by AI tools like ChatGPT. I try to validate the answers by running code snippets or asking follow-up questions during the process so it's not just a blind trust in AI or any other source.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b6c59",
   "metadata": {},
   "source": [
    "## About Python Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590f9702",
   "metadata": {},
   "source": [
    "### What is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14e877",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1a676ea",
   "metadata": {},
   "source": [
    "## About databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89cd0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ab7fe76",
   "metadata": {},
   "source": [
    "## About data structures\n",
    "\n",
    "### Difference between `tokenize` and `vectorize`?\n",
    "- `tokenize` is the process of breaking down text into individual tokens (words, phrases, etc.), which is often the first step in text processing.\n",
    "\n",
    "- `vectorize` is the process of converting these tokens into numerical vectors, which can be used for machine learning or other computational tasks. This often involves techniques like TF-IDF, word embeddings, or one-hot encoding. In TF-IDF, for example, each token is represented by a vector that reflects its importance in the document relative to a corpus of documents. One problem with TF-IDF is that it creates vector space that is too sparse because many tokens will not appear in every document, leading to many zero values in the vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334f600",
   "metadata": {},
   "source": [
    "### What is JSON anyway?\n",
    "JSON (JavaScript Object Notation) is a lightweight data interchange format that is easy for humans to read and write (it is not). In python, for me at least, JSON looks like a dictionary with additional quotation marks. The reason people use JSON is that it can work across different programming languages and platforms, making it a common choice for data exchange in web applications and APIs. It is like something people use different languages to communicate with each other. \n",
    "\n",
    "For example, People around the world can read 0,1,2,3,4,5,6,7,8,9 and understand the meaning of these numbers despite the fact that they speak different languages. JSON is like a universal language for data exchange.\n",
    "\n",
    "To learn how to use JSON in Python, here is a simple and helpful resource from w3schools: [https://www.w3schools.com/python/python_json.asp](https://www.w3schools.com/python/python_json.asp)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
