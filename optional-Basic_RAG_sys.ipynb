{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323d2ecd",
   "metadata": {},
   "source": [
    "# Build a simple RAG system with semantic search and BM25\n",
    "\n",
    "To build a simple RAG, we need build a query function, a retrieval function, and a generation function. We will also need a dataset to retrieve from.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60c8ae",
   "metadata": {},
   "source": [
    "## Understand Data Structures\n",
    "\n",
    "Many tutorials usually use some sort of open-source dataset as examples to show how RAG works. For me, the problem is not really about the RAG workflow, but how to work with data. For beginners like me, it is not a simple question. It is actually the most mind-boggling part of the whole process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff20371",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bm25s\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff444093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bm25s\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adbbb30",
   "metadata": {},
   "source": [
    "First we need to read the dataset. Pandas is a great and most common choice for data manipulation. We can use `pd.read_csv()` to read the dataset into a DataFrame and assign it to a variable. \n",
    "\n",
    "Then we can do some basic data exploration to understand the structure of the dataset. For example, we can use `df.head()` to see the first few rows of the dataset, and `df.info()` to see the data types and null values in each column. Here \"df\" is the DataFrame variable we assigned earlier, it can be any name you choose but somehow people like to use \"df\" as a short form for DataFrame.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d503765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DATA = pd.read_csv(\"./resource/sample_data/news_data_dedup.csv\")\n",
    "NEWS_DATA.head(3) # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9efe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DATA['description'][0] #access the first row of the description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DATA.loc[0] #access the first row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07575ef5",
   "metadata": {},
   "source": [
    "It takes some practice to become comfortable with accessing and manipulating data in Pandas. Don't panic.\n",
    "\n",
    "Pandas library has very good documentation here: https://pandas.pydata.org/\n",
    "\n",
    "And a cheat sheet here: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf . A copy of the cheat sheet is also included in the resource folder of this project. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec829461",
   "metadata": {},
   "source": [
    "## Start with querying\n",
    "\n",
    "If we have a structured dataset, we can start with querying by index. We can create a function that takes an index or a list of indices as input and returns the corresponding rows from the DataFrame. This is a simple way to retrieve data without any complex logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1166ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_news(indices, dataset=NEWS_DATA):\n",
    "    \"\"\"\n",
    "    Retrieves elements from a dataset based on specified indices.\n",
    "\n",
    "    Parameters:\n",
    "    indices (list of int): A list containing the indices of the desired elements in the dataset.\n",
    "    dataset (list or sequence): The dataset from which elements are to be retrieved. It should support indexing.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of elements from the dataset corresponding to the indices provided in list_of_indices.\n",
    "    \"\"\"\n",
    "\n",
    "    output = [dataset.iloc[index] for index in indices]\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "indices = [0, 1, 2]\n",
    "result = query_news(indices)\n",
    "print(result)\n",
    "print(\"Type of result:\", type(result))\n",
    "print(\"=\" * 50)\n",
    "# Display the first element's description from the result\n",
    "print(f\"\"\"Description of {result[0]['title']}:\\n \n",
    "\\\"{result[0]['description']} \\\"\\n \n",
    "Publication date: {result[0]['published_at']}\"\"\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea6fd1",
   "metadata": {},
   "source": [
    "### BM25 Retrieval\n",
    "\n",
    "Now that we can retrieve data by index, we can work on the retrieval function, which ideally should take a query as input and return the indices of the relevant rows from the DataFrame. \n",
    "\n",
    "Let's start with bm25 using `bm25s` library. BM25 is a popular algorithm for information retrieval that ranks documents based on their relevance to a given query. It is widely used in search engines and can be a good starting point for building a retrieval function.\n",
    "\n",
    "First, we will construct the \"corpus\" from the DataFrame. The corpus is a list of strings, where each string is a document in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The corpus used will be the title appended with the description\n",
    "# the \"to_dict('records')\" method converts the DataFrame to a list of dictionaries, where each dictionary represents a row in the DataFrame.\n",
    "records = NEWS_DATA.to_dict('records')\n",
    "print(records[0])  # Display the first record to check the structure\n",
    "corpus = [x['title'] + \" \" + x['description'] for x in records]\n",
    "print(\"Corpus created with\", len(corpus), \"documents. The type of corpus is:\", type(corpus))\n",
    "print(\"First document in the corpus:\", corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2b3c4e",
   "metadata": {},
   "source": [
    "We then need to instantiate the BM25 retriever by passing the corpus data. The `bm25s` library provides a `BM25` class that we can use for this purpose.\n",
    "\n",
    "In the follow code, the `BM25_RETRIEVER` is an instance of the `BM25` class, which is initialized with the corpus. There are some common methods we can use with the `BM25_RETRIEVER` object, such as `index()` to index the tokenized chunks, and `retrieve()` to retrieve relevant documents based on a query.\n",
    "\n",
    "To instantiate the retriever, we first need call the `bm25s.BM25()` constructor with the corpus data. \n",
    "\n",
    "Then we can tokenize the corpus data using the `bm25s.tokenize()` function. This function takes the corpus and an optional list of stopwords as input and returns a list of tokenized documents. Tokenization is the process of breaking down the text into smaller units (tokens) for better processing.\n",
    "\n",
    "The next step is to index the tokenized chunks within the retriever. This will allow the retriever to efficiently search for relevant documents based on a query. The `index()` method is used for this purpose, and it takes the tokenized data as input.\n",
    "\n",
    "After tokenizing and indexing the data, we are ready to use the BM25 retriever to retrieve relevant documents based on a query. The `retrieve()` method can be used to perform the retrieval, and it will return a list of relevant documents along with their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the retriever by passing the corpus data\n",
    "# BM25_RETRIEVER is an instance of the BM25 class from the bm25s library, which is used for information retrieval.\n",
    "# It is initialized with the corpus, which is a list of strings where each string represents a document in the dataset.\n",
    "BM25_RETRIEVER = bm25s.BM25(corpus=corpus)\n",
    "\n",
    "# Define a list of stopwords to be used during tokenization, this is optional but may improve retrieval performance\n",
    "stopwords = [\"a\", \"the\", \"and\", \"is\", \"to\", \"of\", \"in\", \"that\", \"it\", \"for\", \"on\", \"with\", \"as\", \"this\", \"by\", \"at\", \"from\"]\n",
    "# Tokenize the chunks, which means breaking down the text into smaller units (tokens) for better processing\n",
    "tokenized_data = bm25s.tokenize(corpus, stopwords=stopwords)\n",
    "\n",
    "# Index the tokenized chunks within the retriever, this is done automatically when you call the `BM25` constructor, but you can also do it explicitly\n",
    "BM25_RETRIEVER.index(tokenized_data)\n",
    "\n",
    "# Check the content of tokenized_data\n",
    "print(\"Content of tokenized_data:\", tokenized_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the same query used in the previous exercise\n",
    "sample_query = \"What are the recent news about GDP?\"\n",
    "tokenized_sample_query = bm25s.tokenize(sample_query)\n",
    "\n",
    "# Get the retrieved results and their respective scores\n",
    "results, scores = BM25_RETRIEVER.retrieve(tokenized_sample_query, k=5)\n",
    "\n",
    "\"\"\"\n",
    "Note the actual results and scores are 'lists of lists', where each inner list corresponds to a query. \n",
    "Since we only have one query, we can access the first element of each list by calling results[0] and scores[0].\n",
    "\"\"\"\n",
    "\n",
    "for r,s in zip(results[0], scores[0]):\n",
    "    print(f\"Document: {r} \\n Score: {s}\")\n",
    "\n",
    "print(f\"Results for query: {sample_query}\\n\")\n",
    "for doc in results[0]:\n",
    "  print(f\"Document retrieved {corpus.index(doc)} : {doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the doc numbers(indices) to perform fusion ranking later\n",
    "indices_bm25 = [corpus.index(doc) for doc in results[0]]\n",
    "print(\"Indices of retrieved documents:\", indices_bm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14eed4",
   "metadata": {},
   "source": [
    "!NOTE \n",
    "\n",
    "The output from the BM25 retriever will be the same for the same query, regardless of the order of the documents in the corpus and the word order of the query. This is because BM25 is a statistical model that calculates the relevance of each document to the query based on the term frequency and inverse document frequency. In comparison, a semantic search model might produce different results based on the context and meaning of the words in the query and documents. Order of the documents in the corpus and the word order matters for semantic search, but not for BM25.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9186f93",
   "metadata": {},
   "source": [
    "### Semantic Search Retrieval\n",
    "Now that we have a basic understanding of how to retrieve data by index and using BM25, we can move on to semantic search. Semantic search is a more advanced technique that uses natural language processing (NLP) to understand the meaning of the query and the documents in the corpus. It can provide more relevant results than BM25, especially for complex queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cee80",
   "metadata": {},
   "source": [
    "A key component of semantic search is the use of embeddings, which are vector representations of text. These embeddings capture semantic meaning, allowing us to compare text based on context. One common way to measure the similarity between these vectors is through cosine similarity, which calculates how close two vectors are in high-dimensional space. This approach helps in finding content that is contextually similar to the user's query, leading to more accurate and meaningful search results.\n",
    "\n",
    "In contrast, BM25 uses a sparse representation based on keyword matching. During the indexing stage, BM25 tokenizes documents, builds an inverted index (mapping each term to the documents it appears in along with term frequencies), and stores statistics such as document length. During the retrieval stage, BM25 tokenizes the query in the same way, looks up documents containing those terms in the inverted index, and calculates a relevance score using the BM25 formula, which combines term frequency (TF), inverse document frequency (IDF), and length normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb2a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first time, we need to create embeddings for the corpus\n",
    "# for the purpose of this example, we will load pre-computed embeddings\n",
    "EMBEDDINGS = joblib.load(\"./resource/sample_data/embeddings.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b8f29",
   "metadata": {},
   "source": [
    "`joblib` is a library that provides tools for saving and loading Python objects, including NumPy arrays and scikit-learn models. It is particularly useful for efficiently storing large datasets or machine learning models in a binary format, allowing for quick loading and saving without the need to re-compute or re-train.\n",
    "\n",
    "You can find the `joblib` library documentation here: https://joblib.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65491c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-base-en-v1.5\" \n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = \"RAG is awesome\"\n",
    "# Using, but truncating the result to not pollute the output, don't truncate it in the exercise.\n",
    "model.encode(query)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors.\n",
    "\n",
    "    Parameters:\n",
    "    a (np.ndarray): First vector.\n",
    "    b (np.ndarray): Second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"What are the primary colors\"\n",
    "query2 = \"Yellow, red and blue\"\n",
    "query3 = \"Cats are friendly animals\"\n",
    "\n",
    "query1_embed = model.encode(query1)\n",
    "query2_embed = model.encode(query2)\n",
    "query3_embed = model.encode(query3)\n",
    "\n",
    "print(f\"Similarity between '{query1}' and '{query2}' = {cosine_similarity(query1_embed, query2_embed)}\")\n",
    "print(f\"Similarity between '{query1}' and '{query3}' = {cosine_similarity(query1_embed, query3_embed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, embeddings, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform semantic search to find the most relevant documents to a given query.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The search query.\n",
    "    embeddings (np.ndarray): A 2D array where each row is the embedding of a document in the corpus.\n",
    "    model (SentenceTransformer): A pre-trained SentenceTransformer model used to encode the query.\n",
    "    top_k (int): The number of top relevant documents to return.\n",
    "\n",
    "    Returns:\n",
    "    list: Indices of the top_k most relevant documents in the corpus.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the query to get its embedding\n",
    "    query_embedding = model.encode(query)\n",
    "    \n",
    "    # Compute cosine similarities between the query embedding and all document embeddings\n",
    "    # the input embeddings is a 2D array, where each row is a document embedding\n",
    "    # the query_embedding is a 1D array\n",
    "    # we use broadcasting to compute the cosine similarity between the query and all documents\n",
    "    cosine_similarities = np.dot(embeddings, query_embedding) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_embedding))\n",
    "    \n",
    "    # Get the indices of the top_k most similar documents\n",
    "    top_k_indices = np.argsort(cosine_similarities)[-top_k:][::-1]\n",
    "    \n",
    "    return top_k_indices.tolist()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6739c8e5",
   "metadata": {},
   "source": [
    "The broadcasting mechanism in NumPy allows us to perform operations on arrays of different shapes. In this case, we can compute the cosine similarity between the query embedding and all document embeddings without needing to reshape the arrays explicitly. A step-by-step breakdown of the cosine similarity calculation is as follows:\n",
    "\n",
    "1. `dot_product = np.dot(EMBEDDINGS, query_embed)`: This computes the dot product between the query embedding and each document embedding. For this particular model, say if we have 1000 documents and each embedding is a 768-dimensional vector, the `EMBEDDINGS` array would have a shape of (1000, 768), and the `query_embed` would have a shape of (768,). The resulting `dot_product` would be a 1D array of shape (1000,), where each element represents the dot product between the query and a corresponding document.\n",
    "\n",
    "2. `norms = np.linalg.norm(EMBEDDINGS, axis=1) * np.linalg.norm(query_embed)`: This computes the norms of the document embeddings and the query embedding. For 1000 documents, the `norms` array would have a shape of (1000,), where each element represents the norm of a corresponding document embedding multiplied by the norm of the query embedding. The query embedding's norm is a single scalar value and it is the same for all documents, so it can be broadcasted across the document norms.\n",
    "\n",
    "3. `cosine_similarities = dot_product / norms`: This computes the cosine similarities by dividing the dot products by the norms. The result is a 1D array where each element represents the cosine similarity between the query and a corresponding document.\n",
    "\n",
    "demostration of dot matrix multiplication and cosine similarity calculation with  3*2 matrix and 2*1 vector:\n",
    "\n",
    "\n",
    "```math\n",
    "\\begin{align*}\n",
    "\\text{Dot Product} & : \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "7 \\\\\n",
    "8\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 \\cdot 7 + 2 \\cdot 8 \\\\\n",
    "3 \\cdot 7 + 4 \\cdot 8 \\\\\n",
    "5 \\cdot 7 + 6 \\cdot 8\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "23 \\\\\n",
    "83 \\\\\n",
    "143\n",
    "\\end{bmatrix} \\\\\n",
    "\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d6fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"trend of economy\"\n",
    "top_k_indices_semantic = semantic_search(query, EMBEDDINGS, model, top_k=7)\n",
    "print(f\"Top {len(top_k_indices_semantic)} indices for query '{query}': {top_k_indices_semantic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec1aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run bm25 again\n",
    "results, scores = BM25_RETRIEVER.retrieve(bm25s.tokenize(query), k=7)\n",
    "indices_bm25 = [corpus.index(doc) for doc in results[0]]\n",
    "print(\"Indices of retrieved documents:\", indices_bm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8f071",
   "metadata": {},
   "source": [
    "## RRF\n",
    "\n",
    "RRF (Reciprocal Rank Fusion) is a method that combines the results from multiple retrieval models to improve the overall performance of the retrieval system. It works by assigning a score to each document based on its rank in the results of different models, and then combining these scores to produce a final ranking.\n",
    "\n",
    "The RRF algorithm works as follows:\n",
    "1. For each retrieval model, retrieve a ranked list of documents for a given query.\n",
    "\n",
    "2. For each document in the ranked list, assign a score based on its rank. The score is typically calculated as `1 / (k + rank)`, where `k` is a constant (often set to 60) and `rank` is the position of the document in the ranked list.\n",
    "\n",
    "3. Sum the scores for each document across all retrieval models.\n",
    "\n",
    "4. Rank the documents based on their combined scores to produce the final result.\n",
    "\n",
    "The formula for the RRF score of a document `d` is given by:\n",
    "\n",
    "```math\n",
    "\\text{RRF}(d) = \\sum_{i=1}^{N} \\frac{1}{k + \\text{rank}_i(d)}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d19bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(list1, list2, top_k=5, K=50):\n",
    "    \"\"\"\n",
    "    Fuse rank from multiple IR systems using Reciprocal Rank Fusion.\n",
    "\n",
    "    Args:\n",
    "        list1 (list[int]): A list of indices of the top-k documents that match the query.\n",
    "        list2 (list[int]): Another list of indices of the top-k documents that match the query.\n",
    "        top_k (int): The number of top documents to consider from each list for fusion. Defaults to 5.\n",
    "        K (int): A constant used in the RRF formula. Defaults to 60.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of indices of the top-k documents sorted by their RRF scores.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Create a dictionary to store the RRF scores for each document index\n",
    "    rrf_scores = {}\n",
    "\n",
    "    # Iterate over each document list\n",
    "    for lst in [list1, list2]:\n",
    "        # Calculate the RRF score for each document index\n",
    "        for rank, item in enumerate(lst, start=1): # Start = 1 set the first element as 1 and not 0. \n",
    "                                                   # This is a convention on how ranks work (the first element in ranking is denoted by 1 and not 0 as in lists)\n",
    "            # If the item is not in the dictionary, initialize its score to 0\n",
    "            if item not in rrf_scores:\n",
    "                rrf_scores[item] = 0\n",
    "                \n",
    "            # Update the RRF score for each document index using the formula 1 / (rank + K)\n",
    "            current_score = 1/(rank+K)\n",
    "            \n",
    "            #print(f\"Document {item} from {lst} has RRF score: {current_score:.4f}\")\n",
    "            rrf_scores[item] += current_score\n",
    "            #print(f\"Updated RRF score for document {item}: {rrf_scores[item]:.4f}\")\n",
    "\n",
    "    # Sort the document indices based on their RRF scores in descending order\n",
    "    sorted_items = sorted(rrf_scores, key=rrf_scores.get, reverse = True)\n",
    "\n",
    "    # Slice the list to get the top-k document indices\n",
    "    top_k_indices = [int(x) for x in sorted_items[:top_k]]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return top_k_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20932802",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage of reciprocal_rank_fusion\n",
    "top_k_indices_bm25 = indices_bm25[:5]  # Assuming we want the top 5 from BM25\n",
    "top_k_indices_semantic = top_k_indices_semantic[:5]  # Assuming we want the top 5 from semantic search\n",
    "top_k_indices_fused = reciprocal_rank_fusion(top_k_indices_bm25, top_k_indices_semantic, top_k=5)\n",
    "\n",
    "print(f\"Top {len(top_k_indices_bm25)} indices from BM25: {top_k_indices_bm25}\")\n",
    "print(f\"Top {len(top_k_indices_semantic)} indices from semantic search: {top_k_indices_semantic}\")\n",
    "print(f\"Top {len(top_k_indices_fused)} indices after RRF fusion: {top_k_indices_fused}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the documents using the fused indices\n",
    "fused_documents = query_news(top_k_indices_fused, NEWS_DATA)\n",
    "print(\"Fused documents retrieved:\")\n",
    "for doc in fused_documents:\n",
    "\n",
    "    print(f\"Title: {doc['title']},\\n Description: {doc['description']}, Published at: {doc['published_at']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496cd723",
   "metadata": {},
   "source": [
    "### Assemble the Pipeline\n",
    "Now that we have the retrieval functions, we can assemble the pipeline. The pipeline will take a query as input, and retrieve the relevant documents using the BM25 and semantic search retrieval functions. The pipeline will return the retrieved documents for later use in the generation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "24446771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_pipeline(query, top_k=5):\n",
    "    bm25_results, bm25_scores = BM25_RETRIEVER.retrieve(bm25s.tokenize(query), k=top_k)\n",
    "    bm25_indices = [corpus.index(doc) for doc in bm25_results[0]]\n",
    "\n",
    "    semantic_indices = semantic_search(query, EMBEDDINGS, model, top_k=top_k)\n",
    "\n",
    "    fused_indices = reciprocal_rank_fusion(bm25_indices, semantic_indices, top_k=top_k)\n",
    "    \n",
    "    fused_documents = query_news(fused_indices, NEWS_DATA)\n",
    "    \n",
    "    return fused_documents\n",
    "\n",
    "# fused_documents is a list of dictionaries, each dictionary contains the title, description, and published_at of the document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4db3ff",
   "metadata": {},
   "source": [
    "## Assemble the RAG system\n",
    "\n",
    "With the \"context\" for the query ready, we can now assemble the RAG system by combining the retrieval and generation components. \n",
    "\n",
    "We will use OpenAI's `GPT-4o-mini` model for the generation component, which is a powerful language model that can generate human-like text based on the context provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadca334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing\n",
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") # Use correct env var name from your .env or manually set it\n",
    "print(\"OpenAI API Key:\", api_key[:10] + \"...\" if api_key else \"Not found\")  # Only show first 10 chars for security\n",
    "base_url = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")  # Use correct env var name from your .env\n",
    "print(\"OpenAI Base URL:\", base_url)\n",
    "\n",
    "# Initialize OpenAI client with API key and base URL\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a6430",
   "metadata": {},
   "source": [
    "The following function `gen_prompt()` takes a query and context as input, and generates a prompt for the language model. The prompt is formatted to include the query and the context, which will help the model generate a relevant response.\n",
    "\n",
    "Play with the prompt template will affect the quality of the generated response. You can experiment with different prompt templates to see how they affect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "559fdd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "def gen_prompt(prompt, context=None):\n",
    "    # context in this case is a list of dictionaries, each dictionary contains the title, description, and published_at of the document\n",
    "    # convert context to a string\n",
    "    if context:\n",
    "        context = \"\\n\\n\".join([f\"Title: {doc['title']}\\n Description: {doc['description']}\\n Published at: {doc['published_at']} \\n URL: {doc['url']}\" for doc in context])\n",
    "    return f\"\"\"\n",
    "    You are a helpful assistant with access to the following context:\n",
    "    ====\n",
    "    Context: {context}\\n\n",
    "    ====\\n\n",
    "\n",
    "    User query: {prompt};\n",
    "    Use the context to answer the user's question as accurately as possible.\n",
    "    If the context does not provide enough information, politely inform the user that you cannot answer based on the provided context.\n",
    "    \"\"\"\n",
    "\n",
    "# version 2: set up a template outside the function for easier modification\n",
    "# you can add more controls too\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "You are a helpful assistant with access to the following context:\n",
    "\n",
    "======\n",
    "Context: {context}\n",
    "======\n",
    "\n",
    "User query: {query};\n",
    "Use the context to answer the user's question as accurately as possible.\n",
    "If the context does not provide enough information, politely inform the user that you cannot answer based on the provided context.\n",
    "\"\"\"\n",
    "\n",
    "def gen_prompt_v2(query, context=None, template=sys_prompt):\n",
    "    #check if context is a list of dictionaries, if so, convert it to a string\n",
    "   if isinstance(context, list) and all(isinstance(doc, dict) for doc in context):\n",
    "       # Convert each document in the context to a formatted string\n",
    "       formated_context = \"\\n\\n\".join([f\"Title: {doc['title']}\\n Description: {doc['description']}\\n Published at: {doc['published_at']} \\n URL: {doc['url']}\" for doc in context])\n",
    "       print(\"Formatted context for prompt:\\n\", formated_context)  # Debugging line to check the formatted context\n",
    "\n",
    "   else:\n",
    "       # If context is None or not a list of dictionaries, set it to an empty string\n",
    "       formated_context = context\n",
    "   final_prompt = template.format(context=formated_context, query=query)\n",
    "   return final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f2b77619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b5d7b8d82146adbf08f8852cff7590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc5e7396f0a4d17b26d86cd82c8d4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful assistant with access to the following context:\n",
      "\n",
      "======\n",
      "Context: [guid                             f2066c4174cd53513551420d0a4afbab\n",
      "title           China's Gold Consumption Rises on Safe-Haven D...\n",
      "description     Chinese buyers, spooked by a protracted proper...\n",
      "venue                                                         WSJ\n",
      "url             https://www.wsj.com/articles/chinas-gold-consu...\n",
      "published_at                               2024-04-26 09:40:00+00\n",
      "updated_at                          2024-04-27 05:32:46.196296+00\n",
      "Name: 488, dtype: object, guid                             a1e9503e615f7fa4d4ea8fab6cf6c76a\n",
      "title                      Blinken’s Visit to China: What to Know\n",
      "description     Secretary of State Antony J. Blinken is in Chi...\n",
      "venue                                                         NYT\n",
      "url             https://www.nytimes.com/2024/04/25/world/asia/...\n",
      "published_at                               2024-04-25 04:46:13+00\n",
      "updated_at                          2024-04-26 20:03:01.551287+00\n",
      "Name: 97, dtype: object, guid                             5f8541de79bf5f1283ee773fc4263545\n",
      "title           Blinken to warn China against helping Russia i...\n",
      "description     US Secretary of State Antony Blinken will visi...\n",
      "venue                                                          RT\n",
      "url             https://www.rt.com/news/596329-blinken-to-thre...\n",
      "published_at                               2024-04-20 23:50:41+00\n",
      "updated_at                          2024-04-26 14:02:52.398808+00\n",
      "Name: 705, dtype: object, guid                             502653532376c3131e9dedee91e3d2f7\n",
      "title           Here’s what makes Blinken’s job in China espec...\n",
      "description     “Overcapacity” and “dual-purpose trade” are ca...\n",
      "venue                                                          RT\n",
      "url             https://www.rt.com/news/596560-blinken-china-v...\n",
      "published_at                               2024-04-25 22:05:17+00\n",
      "updated_at                          2024-04-26 20:03:01.997589+00\n",
      "Name: 141, dtype: object, guid                             fac31989a0cc100eb78ae75e4fc7b80d\n",
      "title           Blinken says China helping fuel Russian threat...\n",
      "description     The US Secretary of State was speaking to the ...\n",
      "venue                                                         BBC\n",
      "url             https://www.bbc.co.uk/news/world-asia-china-68...\n",
      "published_at                               2024-04-26 13:22:13+00\n",
      "updated_at                          2024-04-27 12:32:43.120864+00\n",
      "Name: 20, dtype: object]\n",
      "======\n",
      "\n",
      "User query: What are the recent news about wheat in China?;\n",
      "Use the context to answer the user's question as accurately as possible.\n",
      "If the context does not provide enough information, politely inform the user that you cannot answer based on the provided context.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maggiezhao/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the recent news about wheat in China?\"\n",
    "context = context_pipeline(query)\n",
    "prompt = gen_prompt_v2(query, context = context)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9cb7ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_with_rag(prompt, api_key=api_key, base_url=base_url, use_RAG=True, top_k=3, print_context=False):\n",
    "\n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    if use_RAG:\n",
    "        context = context_pipeline(query, top_k=top_k)  # Get the context using the index_pipeline function\n",
    "        prompt = gen_prompt_v2(prompt, context=context)  # Use the sys_prompt function to format the prompt with RAG data\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4o-mini-preview\" for the preview version\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "        top_p=1.0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "        if print_context:\n",
    "            print(\"Context used for RAG:\", context)  # Print the context if requested\n",
    "    \n",
    "        return response.choices[0].message.content\n",
    "    else:\n",
    "        # If not using RAG, just return the prompt\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4o-mini-preview\" for the preview version\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "        top_p=1.0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "57d3a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What are the recent news about China?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6632ef2688ea4165ac6b9a35a8e86a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a69ac71c4ba4286b7a8a4b179c7f3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context used for RAG: [guid                             f2066c4174cd53513551420d0a4afbab\n",
      "title           China's Gold Consumption Rises on Safe-Haven D...\n",
      "description     Chinese buyers, spooked by a protracted proper...\n",
      "venue                                                         WSJ\n",
      "url             https://www.wsj.com/articles/chinas-gold-consu...\n",
      "published_at                               2024-04-26 09:40:00+00\n",
      "updated_at                          2024-04-27 05:32:46.196296+00\n",
      "Name: 488, dtype: object, guid                             a1e9503e615f7fa4d4ea8fab6cf6c76a\n",
      "title                      Blinken’s Visit to China: What to Know\n",
      "description     Secretary of State Antony J. Blinken is in Chi...\n",
      "venue                                                         NYT\n",
      "url             https://www.nytimes.com/2024/04/25/world/asia/...\n",
      "published_at                               2024-04-25 04:46:13+00\n",
      "updated_at                          2024-04-26 20:03:01.551287+00\n",
      "Name: 97, dtype: object, guid                             5f8541de79bf5f1283ee773fc4263545\n",
      "title           Blinken to warn China against helping Russia i...\n",
      "description     US Secretary of State Antony Blinken will visi...\n",
      "venue                                                          RT\n",
      "url             https://www.rt.com/news/596329-blinken-to-thre...\n",
      "published_at                               2024-04-20 23:50:41+00\n",
      "updated_at                          2024-04-26 14:02:52.398808+00\n",
      "Name: 705, dtype: object]\n",
      "RAG Assistant:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Recent news about China includes:\n",
       "\n",
       "1. **China's Gold Consumption**: There has been a notable rise in gold consumption among Chinese buyers, driven by concerns over economic stability and a protracted property downturn. This trend reflects a shift towards gold as a safe-haven asset in uncertain times. More details can be found in the article titled \"China's Gold Consumption Rises on Safe-Haven Demand\" published by the Wall Street Journal on April 26, 2024. [Read more here](https://www.wsj.com/articles/chinas-gold-consu).\n",
       "\n",
       "2. **Blinken's Visit to China**: U.S. Secretary of State Antony J. Blinken recently visited China, where he addressed various diplomatic matters. The visit is significant in the context of U.S.-China relations and ongoing global issues. The New York Times reported on this visit on April 25, 2024. [Read more here](https://www.nytimes.com/2024/04/25/world/asia/...).\n",
       "\n",
       "3. **Warnings Against Supporting Russia**: During his visit, Blinken is expected to warn China against providing support to Russia amid the ongoing conflict in Ukraine. This is part of broader discussions on international relations and security concerns. This topic was covered by RT on April 20, 2024. [Read more here](https://www.rt.com/news/596329-blinken-to-thre...).\n",
       "\n",
       "These articles highlight key developments in China's economic behavior and its diplomatic engagements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate response using the LLM with RAG\n",
    " \n",
    "user_input = input(\"You: \")\n",
    "print(\"You:\", user_input)\n",
    "# Initialize messages with the user's input\n",
    "response_generate_llm_with_rag = generate_llm_with_rag(user_input, api_key=api_key, base_url=base_url, use_RAG=True,print_context=True)\n",
    "print(\"RAG Assistant:\")\n",
    "display(Markdown(response_generate_llm_with_rag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeede2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response using the LLM without RAG\n",
    "user_input = input(\"You: \")\n",
    "print(\"You:\", user_input)\n",
    "\n",
    "response_generate_llm_with_rag_no_rag = generate_llm_with_rag(user_input, api_key=api_key, base_url=base_url, use_RAG=False)\n",
    "print(\"Assistant (without RAG):\")\n",
    "display(Markdown(response_generate_llm_with_rag_no_rag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37de90a",
   "metadata": {},
   "source": [
    "=========END========="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
