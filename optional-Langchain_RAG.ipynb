{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efcb229",
   "metadata": {},
   "source": [
    "# Use LangChain to build a RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4811452",
   "metadata": {},
   "source": [
    "LangChain is a framework designed to simplify the development of applications that use large language models (LLMs) and other AI tools. It provides a set of abstractions and utilities that make it easier to build complex applications by chaining together different components, such as LLMs, data sources, and processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e6abeb",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "\n",
    "1. Process Documents with LangChain\n",
    "2. Chunking with LangChain\n",
    "3. Vectorization with LangChain\n",
    "4. Database with LangChain\n",
    "5. Querying with LangChain\n",
    "6. Retrieval with LangChain\n",
    "7. Retrieval-Augmented Generation (RAG) with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eca58b",
   "metadata": {},
   "source": [
    "## Load and Process Documents with LangChain\n",
    "\n",
    "To load and process documents in LangChain, you can use the `DocumentLoader` class. This class provides a unified interface for loading documents from various sources, such as local files, URLs, or databases. You can also apply transformations to the loaded documents, such as text extraction or metadata extraction.\n",
    "\n",
    "https://python.langchain.com/docs/concepts/document_loaders/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d749cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyMuPDFLoader to load documents\n",
    "!pip install langchain-community pymupdf\n",
    "!pip install langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\n",
    "    file_path = \"./resource/pdf/bmjgh-2021-005292.pdf\",\n",
    "    # headers = None\n",
    "    # password = None,\n",
    "    mode = \"single\",\n",
    "    pages_delimiter = \"\\n\\n\"\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata, which is extracted when loading the document\n",
    "documents[0].metadata  # Print metadata of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d9208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "print(documents[0].page_content[:500])  # Print the first 500 characters of the first document\n",
    "print(\"==\" * 20)\n",
    "print(\"Metadata of the first document:\")\n",
    "for key, value in documents[0].metadata.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a309d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "pdf_path = \"./resource/pdf/bmjgh-2021-005292.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "print(\"Embedded metadata:\", doc.metadata, \"\\n\")\n",
    "\n",
    "\n",
    "# 如果 author 为空，尝试从第一页正文粗略猜测\n",
    "if not doc.metadata.get(\"author\"):\n",
    "    first_text = doc[0].get_text()  # 默认提取文字\n",
    "    lines = [l.strip() for l in first_text.splitlines() if l.strip()]\n",
    "    candidates = []\n",
    "    for line in lines:\n",
    "        if line.lower().startswith(\"abstract\"):  # 作者通常在 Abstract 之前\n",
    "            break\n",
    "        # 简单启发：含逗号 / 多个名字且不过长\n",
    "        if (\",\" in line or \" and \" in line.lower()) and 3 <= len(line) <= 180:\n",
    "            candidates.append(line)\n",
    "    print(\"Heuristic author lines:\")\n",
    "    for i, line in enumerate(candidates, 1):\n",
    "        print(f\"{i}. {line}\")\n",
    "else:\n",
    "    print(\"Author metadata already exists:\", doc.metadata[\"author\"])\n",
    "    \n",
    "# 关闭文档\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf3ccc",
   "metadata": {},
   "source": [
    "Once we have the metadata, we can use it implement filtering or other logic in our RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e4341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter logic using metadata\n",
    "def filter_documents(documents_data, author=None, title_keyword=None, year=None, journal=None):\n",
    "    \"\"\"parameters:\n",
    "    documents_data: list of metadata dicts containing 'author', 'title', 'year', \n",
    "    'journal' and index of the document\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for idx, meta in enumerate(documents_data):\n",
    "        if author and (not meta.get(\"author\") or author.lower() not in meta[\"author\"].lower()):\n",
    "            continue\n",
    "        if title_keyword and (not meta.get(\"title\") or title_keyword.lower() not in meta[\"title\"].lower()):\n",
    "            continue\n",
    "        if year and (not meta.get(\"year\") or str(year) != str(meta[\"year\"])):\n",
    "            continue\n",
    "        if journal and (not meta.get(\"journal\") or journal.lower() not in meta[\"journal\"].lower()):\n",
    "            continue\n",
    "        filtered.append(idx)\n",
    "\n",
    "    # return indices of documents that match the criteria\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd52470",
   "metadata": {},
   "source": [
    "### Assign stable UUIDs to documents and chunks\n",
    "\n",
    "- For each document: generate a deterministic `doc_id` (UUID v5) using DOI if available, otherwise a hash of source + first 256 chars.\n",
    "- For each chunk: generate a deterministic `chunk_id` (UUID v5) using `doc_id`, chunk index, and a short hash of the chunk text.\n",
    "\n",
    "This ensures IDs are stable across runs if the inputs do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1eed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid, hashlib\n",
    "from typing import List, Dict\n",
    "\n",
    "NAMESPACE = uuid.UUID(\"12345678-1234-5678-1234-567812345678\")  # project-level constant\n",
    "\n",
    "def stable_doc_id(meta: Dict, content_preview: str = \"\") -> str:\n",
    "    \"\"\"Create a deterministic UUID for a document.\n",
    "    Priority: DOI -> URL -> (source + preview hash)\n",
    "    \"\"\"\n",
    "    key = (\n",
    "        meta.get(\"doi\")\n",
    "        or meta.get(\"url\")\n",
    "        or f\"{meta.get('source', 'unknown')}::{hashlib.sha1(content_preview.encode('utf-8')).hexdigest()[:12]}\"\n",
    "    )\n",
    "    return str(uuid.uuid5(NAMESPACE, key))\n",
    "\n",
    "def stable_chunk_id(doc_id: str, idx: int, text: str) -> str:\n",
    "    sig = f\"{doc_id}:{idx}:{hashlib.sha1(text.encode('utf-8')).hexdigest()[:12]}\"\n",
    "    return str(uuid.uuid5(NAMESPACE, sig))\n",
    "\n",
    "# Example: assign ids to loaded LangChain documents and their chunks\n",
    "# Assume `documents` is a list of Document objects from LangChain (with .page_content and .metadata)\n",
    "\n",
    "doc_records = []\n",
    "for d in documents:\n",
    "    preview = d.page_content[:256] if d.page_content else \"\"\n",
    "    d.metadata = dict(d.metadata) if d.metadata else {}\n",
    "    d.metadata[\"doc_id\"] = stable_doc_id(d.metadata, preview)\n",
    "    doc_records.append({\"doc_id\": d.metadata[\"doc_id\"], **d.metadata})\n",
    "\n",
    "print(\"Assigned document IDs (first 3):\", [r[\"doc_id\"] for r in doc_records[:3]])\n",
    "\n",
    "# If you chunk later, reuse doc_id and generate chunk_id per chunk\n",
    "# Example chunking (simple):\n",
    "\n",
    "def simple_chunk(text: str, size: int = 800, overlap: int = 100) -> List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + size)\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    return chunks\n",
    "\n",
    "chunk_records = []\n",
    "for d in documents:\n",
    "    doc_id = d.metadata[\"doc_id\"]\n",
    "    chunks = simple_chunk(d.page_content)\n",
    "    for i, ch in enumerate(chunks):\n",
    "        chunk_id = stable_chunk_id(doc_id, i, ch)\n",
    "        chunk_records.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"chunk_index\": i,\n",
    "            \"text\": ch\n",
    "        })\n",
    "\n",
    "print(\"Sample chunk IDs:\", [c[\"chunk_id\"] for c in chunk_records[:5]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
